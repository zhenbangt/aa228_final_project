{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of leduc_holdem_cfr.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhenbangt/aa228_final_project/blob/main/colab-testing-mccfr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miBl4S8JARzX"
      },
      "source": [
        "\n",
        "\n",
        "# <a href='https://github.com/datamllab/rlcard'> <center> <img src='https://miro.medium.com/max/1000/1*_9abDpNTM9Cbsd2HEXYm9Q.png' width=500 class='center' /></a> \n",
        "\n",
        "## **Training CFR on Leduc Hold'em**\n",
        "To show how we can use `step` and `step_back` to traverse the game tree, we provide an example of solving Leduc Hold'em with CFR:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvSQRtFHfQde"
      },
      "source": [
        "* First, we install RLcard and Tensorflow. To use Tensorflow implementation of the example algorithms, we recommend installing the supported verison of Tensorflow with rlcard[tensorflow]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQ8CiXAJjQGi",
        "outputId": "a746bf7f-8aeb-455c-af18-10340059942a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install -q rlcard \n",
        "!pip install -q rlcard[tensorflow]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 6.7MB 12.1MB/s \n",
            "\u001b[?25h  Building wheel for rlcard (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 110.5MB 54kB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 8.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 512kB 53.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.8MB 60.3MB/s \n",
            "\u001b[?25h  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: tensorflow-probability 0.11.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_8Kuf47kghG"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import rlcard\n",
        "from rlcard.agents import CFRAgent\n",
        "from rlcard import models\n",
        "from rlcard.utils import set_global_seed, tournament\n",
        "from rlcard.utils import Logger\n",
        "import collections\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "from rlcard.utils.utils import *\n",
        "\n",
        "# Make environment and enable human mode\n",
        "env = rlcard.make('leduc-holdem', config={'seed': 0, 'allow_step_back':True})\n",
        "eval_env = rlcard.make('leduc-holdem', config={'seed': 0})"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOj7sRRRx6WX"
      },
      "source": [
        "class MCCFRAgent:\n",
        "    def __init__(self, env, model_path=\"./cfr_model\"):\n",
        "        \"\"\" \n",
        "            env: Simulator from RLcard\n",
        "        \"\"\"\n",
        "        self.use_raw = False  # use_raw is indicator to the rlcard env\n",
        "        self.env = env\n",
        "        self.model_path = model_path\n",
        "\n",
        "        # A policy is a dict state_str -> action probabilities\n",
        "        self.policy = collections.defaultdict(list)\n",
        "        self.average_policy = collections.defaultdict(np.array)\n",
        "        # Regret is a dict state_str -> action regrets\n",
        "        self.regrets = collections.defaultdict(np.array)\n",
        "        self.iteration = 0\n",
        "\n",
        "    def train(self):\n",
        "        self.iteration += 1\n",
        "        # Firstly, traverse tree to compute counterfactual regret for each player\n",
        "        # The regrets are recorded in traversal\n",
        "        for player_id in range(self.env.player_num):\n",
        "            self.env.reset()\n",
        "            probs = np.ones(self.env.player_num)\n",
        "            self.traverse_tree(probs, player_id)\n",
        "\n",
        "        # For MCCFR update policy after each traversal?\n",
        "        self.update_policy()\n",
        "\n",
        "    def traverse_tree(self, probs, player_id):\n",
        "        \"\"\" Traverse the game tree, update the regrets\n",
        "        Args:\n",
        "            probs: The reach of the current node (probability given other player actions)\n",
        "            player_id: The identified player to update the value\n",
        "        Returns:\n",
        "            state_utility (list): The expected utilities for all the players\n",
        "        \"\"\"\n",
        "        if self.env.is_over():\n",
        "            return self.env.get_payoffs()\n",
        "\n",
        "        # determine who's turn\n",
        "        current_player = self.env.get_player_id()\n",
        "        # action_utilities is for current player; state_utility is for all players\n",
        "        action_utilities = dict()\n",
        "        state_utility = np.zeros(self.env.player_num)\n",
        "        obs, legal_actions = self.get_state(current_player)\n",
        "        # policies are proportional to postive regret\n",
        "        action_probs = self.action_probs(obs, legal_actions, self.policy)\n",
        "\n",
        "        if current_player != player_id:\n",
        "            # for MCCFR this should be a an action according to the policy of current player?\n",
        "            # sample an action\n",
        "            action = np.random.default_rng().choice(range(self.env.action_num), p=action_probs)\n",
        "            self.env.step(action)\n",
        "            utility = self.traverse_tree(probs, player_id)\n",
        "            self.env.step_back()\n",
        "            ################## ? #################\n",
        "            if obs not in self.average_policy:\n",
        "                self.average_policy[obs] = np.zeros(self.env.action_num)\n",
        "            ######################################\n",
        "            self.average_policy[obs][action] +=  self.iteration * 1\n",
        "            return utility\n",
        "\n",
        "        # if current player is the identified player\n",
        "        else:\n",
        "            for action in legal_actions:\n",
        "                action_prob = action_probs[action]\n",
        "                # integers are not mutable; do not need to use deepcopy\n",
        "                new_probs = np.copy(probs)\n",
        "                new_probs[current_player] *= action_prob\n",
        "\n",
        "                # Keep traversing the child state to get utility of the state after taking this action\n",
        "                self.env.step(action)\n",
        "                utility = self.traverse_tree(new_probs, player_id)\n",
        "                self.env.step_back()\n",
        "\n",
        "                # calculate expected state utility (vectorised for all agents)\n",
        "                state_utility += action_prob * utility\n",
        "                # store the \"Q-value\"\n",
        "                action_utilities[action] = utility\n",
        "\n",
        "            # If it is the identified player, we record the policy and compute regret\n",
        "            # counterfactual_prob a.k.a reach (excluding self contribution)\n",
        "            \n",
        "            player_prob = probs[current_player] # self-contribtuion\n",
        "            counterfactual_prob = np.prod(probs[:current_player]) * np.prod(probs[current_player + 1 :])\n",
        "            player_state_utility = state_utility[current_player]\n",
        "\n",
        "            # if this information set has not been discovered before:\n",
        "            if obs not in self.regrets:\n",
        "                self.regrets[obs] = np.zeros(self.env.action_num)\n",
        "            if obs not in self.average_policy:\n",
        "                self.average_policy[obs] = np.zeros(self.env.action_num)\n",
        "\n",
        "            # compute the advantage (regret) for each action from the current policy\n",
        "            for action in legal_actions:\n",
        "                action_prob = action_probs[action]\n",
        "                regret = counterfactual_prob * (action_utilities[action][current_player] - player_state_utility)\n",
        "                regret = action_utilities[action][current_player] - player_state_utility\n",
        "                self.regrets[obs][action] += regret\n",
        "                self.average_policy[obs][action] += self.iteration * player_prob * action_prob\n",
        "            return state_utility\n",
        "\n",
        "    def update_policy(self):\n",
        "        \"\"\" Apply regret matching to each infoset\n",
        "        \"\"\"\n",
        "        for obs in self.regrets:\n",
        "            self.policy[obs] = self.regret_matching(obs)\n",
        "\n",
        "    def regret_matching(self, obs):\n",
        "        \"\"\" Apply regret matching to an infoset\n",
        "        \"\"\"\n",
        "        regret = self.regrets[obs]\n",
        "        positive_regret_sum = sum([r for r in regret if r > 0])\n",
        "\n",
        "        action_probs = np.zeros(self.env.action_num)\n",
        "        if positive_regret_sum > 0:\n",
        "            for action in range(self.env.action_num):\n",
        "                action_probs[action] = max(0.0, regret[action] / positive_regret_sum)\n",
        "        else:\n",
        "            for action in range(self.env.action_num):\n",
        "                action_probs[action] = 1.0 / self.env.action_num\n",
        "        return action_probs\n",
        "\n",
        "    def action_probs(self, obs, legal_actions, policy):\n",
        "        \"\"\" Obtain the action probabilities of the current state\n",
        "        Args:\n",
        "            obs (str): state_str\n",
        "            legal_actions (list): List of leagel actions\n",
        "            policy (dict): The used policy\n",
        "        Returns:\n",
        "            (tuple) that contains:\n",
        "                action_probs(numpy.array): The action probabilities\n",
        "                legal_actions (list): Indices of legal actions\n",
        "        \"\"\"\n",
        "        if obs not in policy.keys():\n",
        "            # uniform\n",
        "            action_probs = np.array([1.0 / self.env.action_num for _ in range(self.env.action_num)])\n",
        "            self.policy[obs] = action_probs\n",
        "        else:\n",
        "            action_probs = policy[obs]\n",
        "        # prune illegal actions\n",
        "        action_probs = remove_illegal(action_probs, legal_actions)\n",
        "        return action_probs\n",
        "\n",
        "    def eval_step(self, state):\n",
        "        \"\"\" Given a state, predict action based on average policy\n",
        "        Args:\n",
        "            state (numpy.array): State representation\n",
        "        Returns:\n",
        "            action (int): Predicted action\n",
        "        \"\"\"\n",
        "        probs = self.action_probs(state[\"obs\"].tostring(), state[\"legal_actions\"], self.average_policy)\n",
        "        action = np.random.choice(len(probs), p=probs)\n",
        "        return action, probs\n",
        "\n",
        "    def get_state(self, player_id):\n",
        "        \"\"\" Get state_str of the player\n",
        "        Args:\n",
        "            player_id (int): The player id\n",
        "        Returns:\n",
        "            (tuple) that contains:\n",
        "                state (str): The state str\n",
        "                legal_actions (list): Indices of legal actions\n",
        "        \"\"\"\n",
        "        state = self.env.get_state(player_id)\n",
        "        return state[\"obs\"].tostring(), state[\"legal_actions\"]\n",
        "\n",
        "    def save(self):\n",
        "        \"\"\" Save model\n",
        "        \"\"\"\n",
        "        if not os.path.exists(self.model_path):\n",
        "            os.makedirs(self.model_path)\n",
        "\n",
        "        policy_file = open(os.path.join(self.model_path, \"policy.pkl\"), \"wb\")\n",
        "        pickle.dump(self.policy, policy_file)\n",
        "        policy_file.close()\n",
        "\n",
        "        average_policy_file = open(os.path.join(self.model_path, \"average_policy.pkl\"), \"wb\")\n",
        "        pickle.dump(self.average_policy, average_policy_file)\n",
        "        average_policy_file.close()\n",
        "\n",
        "        regrets_file = open(os.path.join(self.model_path, \"regrets.pkl\"), \"wb\")\n",
        "        pickle.dump(self.regrets, regrets_file)\n",
        "        regrets_file.close()\n",
        "\n",
        "        iteration_file = open(os.path.join(self.model_path, \"iteration.pkl\"), \"wb\")\n",
        "        pickle.dump(self.iteration, iteration_file)\n",
        "        iteration_file.close()\n",
        "\n",
        "    def load(self):\n",
        "        \"\"\" Load model\n",
        "        \"\"\"\n",
        "        if not os.path.exists(self.model_path):\n",
        "            return\n",
        "\n",
        "        policy_file = open(os.path.join(self.model_path, \"policy.pkl\"), \"rb\")\n",
        "        self.policy = pickle.load(policy_file)\n",
        "        policy_file.close()\n",
        "\n",
        "        average_policy_file = open(os.path.join(self.model_path, \"average_policy.pkl\"), \"rb\")\n",
        "        self.average_policy = pickle.load(average_policy_file)\n",
        "        average_policy_file.close()\n",
        "\n",
        "        regrets_file = open(os.path.join(self.model_path, \"regrets.pkl\"), \"rb\")\n",
        "        self.regrets = pickle.load(regrets_file)\n",
        "        regrets_file.close()\n",
        "\n",
        "        iteration_file = open(os.path.join(self.model_path, \"iteration.pkl\"), \"rb\")\n",
        "        self.iteration = pickle.load(iteration_file)\n",
        "        iteration_file.close()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N23rzmm9yfUG"
      },
      "source": [
        "* Now we start to train CFR on Lecuc Hold'em. The training logs and the learning curves are shown as below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNB9GM9nqk6Q",
        "outputId": "0fbd1e66-1dc2-4eb2-8d36-b0eb286b6fd6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Set the iterations numbers and how frequently we evaluate/save plot\n",
        "evaluate_every = 100\n",
        "save_plot_every = 1000\n",
        "evaluate_num = 10000\n",
        "episode_num = 10000\n",
        "\n",
        "# The paths for saving the logs and learning curves\n",
        "log_dir = './experiments/leduc_holdem_cfr_result/'\n",
        "\n",
        "# Set a global seed\n",
        "set_global_seed(0)\n",
        "\n",
        "# Initilize CFR Agent\n",
        "agent = MCCFRAgent(env)\n",
        "# agent.load()  # If we have saved model, we first load the model\n",
        "\n",
        "# Evaluate CFR against pre-trained NFSP\n",
        "eval_env.set_agents([agent, models.load('leduc-holdem-nfsp').agents[0]])\n",
        "\n",
        "# Init a Logger to plot the learning curve\n",
        "logger = Logger(log_dir)\n",
        "\n",
        "for episode in range(episode_num):\n",
        "    agent.train()\n",
        "    print('\\rIteration {}'.format(episode), end='')\n",
        "    # Evaluate the performance. Play with NFSP agents.\n",
        "    if episode % evaluate_every == 0:\n",
        "        agent.save() # Save model\n",
        "        logger.log_performance(env.timestep, tournament(eval_env, evaluate_num)[0])\n",
        "\n",
        "# Close files in the logger\n",
        "logger.close_files()\n",
        "\n",
        "# Plot the learning curve\n",
        "logger.plot('CFR')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /usr/local/lib/python3.6/dist-packages/rlcard/models/pretrained/leduc_holdem_nfsp/model\n",
            "Iteration 0\n",
            "----------------------------------------\n",
            "  timestep     |  103646\n",
            "  reward       |  -0.83765\n",
            "----------------------------------------\n",
            "Iteration 100\n",
            "----------------------------------------\n",
            "  timestep     |  106730\n",
            "  reward       |  0.2187\n",
            "----------------------------------------\n",
            "Iteration 200\n",
            "----------------------------------------\n",
            "  timestep     |  110051\n",
            "  reward       |  0.3366\n",
            "----------------------------------------\n",
            "Iteration 300\n",
            "----------------------------------------\n",
            "  timestep     |  113510\n",
            "  reward       |  0.36455\n",
            "----------------------------------------\n",
            "Iteration 400\n",
            "----------------------------------------\n",
            "  timestep     |  116949\n",
            "  reward       |  0.3827\n",
            "----------------------------------------\n",
            "Iteration 500\n",
            "----------------------------------------\n",
            "  timestep     |  119973\n",
            "  reward       |  0.36235\n",
            "----------------------------------------\n",
            "Iteration 600\n",
            "----------------------------------------\n",
            "  timestep     |  123076\n",
            "  reward       |  0.33765\n",
            "----------------------------------------\n",
            "Iteration 700\n",
            "----------------------------------------\n",
            "  timestep     |  126461\n",
            "  reward       |  0.3814\n",
            "----------------------------------------\n",
            "Iteration 800\n",
            "----------------------------------------\n",
            "  timestep     |  129917\n",
            "  reward       |  0.413\n",
            "----------------------------------------\n",
            "Iteration 900\n",
            "----------------------------------------\n",
            "  timestep     |  133677\n",
            "  reward       |  0.4708\n",
            "----------------------------------------\n",
            "Iteration 1000\n",
            "----------------------------------------\n",
            "  timestep     |  137680\n",
            "  reward       |  0.45235\n",
            "----------------------------------------\n",
            "Iteration 1100\n",
            "----------------------------------------\n",
            "  timestep     |  141649\n",
            "  reward       |  0.47265\n",
            "----------------------------------------\n",
            "Iteration 1200\n",
            "----------------------------------------\n",
            "  timestep     |  145410\n",
            "  reward       |  0.45645\n",
            "----------------------------------------\n",
            "Iteration 1300\n",
            "----------------------------------------\n",
            "  timestep     |  149182\n",
            "  reward       |  0.48505\n",
            "----------------------------------------\n",
            "Iteration 1400\n",
            "----------------------------------------\n",
            "  timestep     |  152843\n",
            "  reward       |  0.5454\n",
            "----------------------------------------\n",
            "Iteration 1500"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlyQo-4CosPS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
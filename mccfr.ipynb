{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import rlcard\n",
    "from rlcard.agents import CFRAgent\n",
    "from rlcard import models\n",
    "from rlcard.utils import set_global_seed, tournament\n",
    "from rlcard.utils import Logger\n",
    "import collections\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from rlcard.utils.utils import *\n",
    "\n",
    "# Make environment and enable human mode\n",
    "env = rlcard.make(\"leduc-holdem\", config={\"seed\": 0, \"allow_step_back\": True})\n",
    "eval_env = rlcard.make(\"leduc-holdem\", config={\"seed\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCCFRAgent:\n",
    "    def __init__(self, env, model_path=\"./cfr_model\"):\n",
    "        \"\"\" \n",
    "            env: Simulator from RLcard\n",
    "        \"\"\"\n",
    "        self.use_raw = False  # use_raw is indicator to the rlcard env\n",
    "        self.env = env\n",
    "        self.model_path = model_path\n",
    "\n",
    "        # A policy is a dict state_str -> action probabilities\n",
    "        self.policy = collections.defaultdict(list)\n",
    "        self.average_policy = collections.defaultdict(np.array)\n",
    "        # Regret is a dict state_str -> action regrets\n",
    "        self.regrets = collections.defaultdict(np.array)\n",
    "        self.iteration = 0\n",
    "\n",
    "    def train(self):\n",
    "        self.iteration += 1\n",
    "        # Firstly, traverse tree to compute counterfactual regret for each player\n",
    "        # The regrets are recorded in traversal\n",
    "        for player_id in range(self.env.player_num):\n",
    "            self.env.reset()\n",
    "            probs = np.ones(self.env.player_num)\n",
    "            self.traverse_tree(probs, player_id)\n",
    "\n",
    "        # For MCCFR update policy after each traversal?\n",
    "        self.update_policy()\n",
    "\n",
    "    def traverse_tree(self, probs, player_id):\n",
    "        \"\"\" Traverse the game tree, update the regrets\n",
    "        Args:\n",
    "            probs: The reach of the current node (probability given other player actions)\n",
    "            player_id: The identified player to update the value\n",
    "        Returns:\n",
    "            state_utility (list): The expected utilities for all the players\n",
    "        \"\"\"\n",
    "        if self.env.is_over():\n",
    "            return self.env.get_payoffs()\n",
    "\n",
    "        # determine who's turn\n",
    "        current_player = self.env.get_player_id()\n",
    "        # action_utilities is for current player; state_utility is for all players\n",
    "        action_utilities = dict()\n",
    "        state_utility = np.zeros(self.env.player_num)\n",
    "        obs, legal_actions = self.get_state(current_player)\n",
    "        # policies are proportional to postive regret\n",
    "        action_probs = self.action_probs(obs, legal_actions, self.policy)\n",
    "\n",
    "        if current_player != player_id:\n",
    "            # for MCCFR this should be a an action according to the policy of current player?\n",
    "            # sample an action\n",
    "            action = np.random.default_rng().choice(range(self.env.action_num), p=action_probs)\n",
    "            self.env.step(action)\n",
    "            utility = self.traverse_tree(probs, player_id)\n",
    "            self.env.step_back()\n",
    "            ################## ? #################\n",
    "            if obs not in self.average_policy:\n",
    "                self.average_policy[obs] = np.zeros(self.env.action_num)\n",
    "            ######################################\n",
    "            self.average_policy[obs][action] +=  self.iteration * 1\n",
    "            return utility\n",
    "\n",
    "        # if current player is the identified player\n",
    "        else:\n",
    "            for action in legal_actions:\n",
    "                action_prob = action_probs[action]\n",
    "                # integers are not mutable; do not need to use deepcopy\n",
    "                new_probs = np.copy(probs)\n",
    "                new_probs[current_player] *= action_prob\n",
    "\n",
    "                # Keep traversing the child state to get utility of the state after taking this action\n",
    "                self.env.step(action)\n",
    "                utility = self.traverse_tree(new_probs, player_id)\n",
    "                self.env.step_back()\n",
    "\n",
    "                # calculate expected state utility (vectorised for all agents)\n",
    "                state_utility += action_prob * utility\n",
    "                # store the \"Q-value\"\n",
    "                action_utilities[action] = utility\n",
    "\n",
    "            # If it is the identified player, we record the policy and compute regret\n",
    "            # counterfactual_prob a.k.a reach (excluding self contribution)\n",
    "            \n",
    "            player_prob = probs[current_player] # self-contribtuion\n",
    "            counterfactual_prob = np.prod(probs[:current_player]) * np.prod(probs[current_player + 1 :])\n",
    "            player_state_utility = state_utility[current_player]\n",
    "\n",
    "            # if this information set has not been discovered before:\n",
    "            if obs not in self.regrets:\n",
    "                self.regrets[obs] = np.zeros(self.env.action_num)\n",
    "            if obs not in self.average_policy:\n",
    "                self.average_policy[obs] = np.zeros(self.env.action_num)\n",
    "\n",
    "            # compute the advantage (regret) for each action from the current policy\n",
    "            for action in legal_actions:\n",
    "                action_prob = action_probs[action]\n",
    "                regret = counterfactual_prob * (action_utilities[action][current_player] - player_state_utility)\n",
    "                regret = action_utilities[action][current_player] - player_state_utility\n",
    "                self.regrets[obs][action] += regret\n",
    "                self.average_policy[obs][action] += self.iteration * player_prob * action_prob\n",
    "            return state_utility\n",
    "\n",
    "    def update_policy(self):\n",
    "        \"\"\" Apply regret matching to each infoset\n",
    "        \"\"\"\n",
    "        for obs in self.regrets:\n",
    "            self.policy[obs] = self.regret_matching(obs)\n",
    "\n",
    "    def regret_matching(self, obs):\n",
    "        \"\"\" Apply regret matching to an infoset\n",
    "        \"\"\"\n",
    "        regret = self.regrets[obs]\n",
    "        positive_regret_sum = sum([r for r in regret if r > 0])\n",
    "\n",
    "        action_probs = np.zeros(self.env.action_num)\n",
    "        if positive_regret_sum > 0:\n",
    "            for action in range(self.env.action_num):\n",
    "                action_probs[action] = max(0.0, regret[action] / positive_regret_sum)\n",
    "        else:\n",
    "            for action in range(self.env.action_num):\n",
    "                action_probs[action] = 1.0 / self.env.action_num\n",
    "        return action_probs\n",
    "\n",
    "    def action_probs(self, obs, legal_actions, policy):\n",
    "        \"\"\" Obtain the action probabilities of the current state\n",
    "        Args:\n",
    "            obs (str): state_str\n",
    "            legal_actions (list): List of leagel actions\n",
    "            policy (dict): The used policy\n",
    "        Returns:\n",
    "            (tuple) that contains:\n",
    "                action_probs(numpy.array): The action probabilities\n",
    "                legal_actions (list): Indices of legal actions\n",
    "        \"\"\"\n",
    "        if obs not in policy.keys():\n",
    "            # uniform\n",
    "            action_probs = np.array([1.0 / self.env.action_num for _ in range(self.env.action_num)])\n",
    "            self.policy[obs] = action_probs\n",
    "        else:\n",
    "            action_probs = policy[obs]\n",
    "        # prune illegal actions\n",
    "        action_probs = remove_illegal(action_probs, legal_actions)\n",
    "        return action_probs\n",
    "\n",
    "    def eval_step(self, state):\n",
    "        \"\"\" Given a state, predict action based on average policy\n",
    "        Args:\n",
    "            state (numpy.array): State representation\n",
    "        Returns:\n",
    "            action (int): Predicted action\n",
    "        \"\"\"\n",
    "        probs = self.action_probs(state[\"obs\"].tostring(), state[\"legal_actions\"], self.average_policy)\n",
    "        action = np.random.choice(len(probs), p=probs)\n",
    "        return action, probs\n",
    "\n",
    "    def get_state(self, player_id):\n",
    "        \"\"\" Get state_str of the player\n",
    "        Args:\n",
    "            player_id (int): The player id\n",
    "        Returns:\n",
    "            (tuple) that contains:\n",
    "                state (str): The state str\n",
    "                legal_actions (list): Indices of legal actions\n",
    "        \"\"\"\n",
    "        state = self.env.get_state(player_id)\n",
    "        return state[\"obs\"].tostring(), state[\"legal_actions\"]\n",
    "\n",
    "    def save(self):\n",
    "        \"\"\" Save model\n",
    "        \"\"\"\n",
    "        if not os.path.exists(self.model_path):\n",
    "            os.makedirs(self.model_path)\n",
    "\n",
    "        policy_file = open(os.path.join(self.model_path, \"policy.pkl\"), \"wb\")\n",
    "        pickle.dump(self.policy, policy_file)\n",
    "        policy_file.close()\n",
    "\n",
    "        average_policy_file = open(os.path.join(self.model_path, \"average_policy.pkl\"), \"wb\")\n",
    "        pickle.dump(self.average_policy, average_policy_file)\n",
    "        average_policy_file.close()\n",
    "\n",
    "        regrets_file = open(os.path.join(self.model_path, \"regrets.pkl\"), \"wb\")\n",
    "        pickle.dump(self.regrets, regrets_file)\n",
    "        regrets_file.close()\n",
    "\n",
    "        iteration_file = open(os.path.join(self.model_path, \"iteration.pkl\"), \"wb\")\n",
    "        pickle.dump(self.iteration, iteration_file)\n",
    "        iteration_file.close()\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\" Load model\n",
    "        \"\"\"\n",
    "        if not os.path.exists(self.model_path):\n",
    "            return\n",
    "\n",
    "        policy_file = open(os.path.join(self.model_path, \"policy.pkl\"), \"rb\")\n",
    "        self.policy = pickle.load(policy_file)\n",
    "        policy_file.close()\n",
    "\n",
    "        average_policy_file = open(os.path.join(self.model_path, \"average_policy.pkl\"), \"rb\")\n",
    "        self.average_policy = pickle.load(average_policy_file)\n",
    "        average_policy_file.close()\n",
    "\n",
    "        regrets_file = open(os.path.join(self.model_path, \"regrets.pkl\"), \"rb\")\n",
    "        self.regrets = pickle.load(regrets_file)\n",
    "        regrets_file.close()\n",
    "\n",
    "        iteration_file = open(os.path.join(self.model_path, \"iteration.pkl\"), \"rb\")\n",
    "        self.iteration = pickle.load(iteration_file)\n",
    "        iteration_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the iterations numbers and how frequently we evaluate/save plot\n",
    "evaluate_every = 100\n",
    "save_plot_every = 1000\n",
    "evaluate_num = 10000\n",
    "episode_num = 10000\n",
    "\n",
    "# The paths for saving the logs and learning curves\n",
    "log_dir = './experiments/leduc_holdem_cfr_result/'\n",
    "\n",
    "# Set a global seed\n",
    "set_global_seed(0)\n",
    "\n",
    "# Initilize CFR Agent\n",
    "agent = MCCFRAgent(env)\n",
    "# agent.load()  # If we have saved model, we first load the model\n",
    "\n",
    "# Evaluate CFR against pre-trained NFSP\n",
    "eval_env.set_agents([agent, models.load('leduc-holdem-nfsp').agents[0]])\n",
    "\n",
    "# Init a Logger to plot the learning curve\n",
    "logger = Logger(log_dir)\n",
    "\n",
    "for episode in range(episode_num):\n",
    "    agent.train()\n",
    "    print('\\rIteration {}'.format(episode), end='')\n",
    "    # Evaluate the performance. Play with NFSP agents.\n",
    "    if episode % evaluate_every == 0:\n",
    "        agent.save() # Save model\n",
    "        logger.log_performance(env.timestep, tournament(eval_env, evaluate_num)[0])\n",
    "\n",
    "# Close files in the logger\n",
    "logger.close_files()\n",
    "\n",
    "# Plot the learning curve\n",
    "logger.plot('CFR')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

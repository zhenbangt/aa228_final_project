{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/zhenbangt/aa228_final_project/blob/main/MCCFR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9DqVVm7cFDQh"
   },
   "source": [
    "# Solving Poker using MCCFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oOwrJYpkFw-z",
    "outputId": "3fd24117-7e58-4e20-8b65-fe9f31189a8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env.py\tsample_data\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "PUr5gQovFDQh"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L0FcxH2fF6Tv",
    "outputId": "fd8a05ee-a8ae-4249-c8e5-fc8a824a71eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 6.7MB 4.2MB/s \n",
      "\u001b[?25h  Building wheel for rlcard (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[K     |████████████████████████████████| 110.5MB 36kB/s \n",
      "\u001b[K     |████████████████████████████████| 512kB 54.2MB/s \n",
      "\u001b[K     |████████████████████████████████| 3.8MB 55.8MB/s \n",
      "\u001b[K     |████████████████████████████████| 51kB 7.8MB/s \n",
      "\u001b[?25h  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: tensorflow-probability 0.11.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q rlcard\n",
    "!pip install -q rlcard[tensorflow]\n",
    "!pip install -q tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "id": "Sj4k3yfuFDQk",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import rlcard\n",
    "from pprint import pprint\n",
    "from rlcard import models\n",
    "from rlcard.agents import RandomAgent\n",
    "from rlcard.agents.best_response_agent import BRAgent\n",
    "from rlcard.core import Card\n",
    "from rlcard.utils import tournament\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "from env import LeducholdemEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "r_GSYHYCFDQn"
   },
   "outputs": [],
   "source": [
    "HANDS = [\"SJ\", \"SQ\", \"SK\", \"HJ\", \"HQ\", \"HK\"]\n",
    "HAND2IDX = {hand: i for i, hand in enumerate(HANDS)}\n",
    "\n",
    "\n",
    "def get_payoff(env, hand, our_id):\n",
    "    orig_hand = copy(env.game.players[1 - our_id].hand)\n",
    "    env.game.players[1 - our_id].hand = Card(suit=hand[0], rank=hand[1])\n",
    "    payoffs = env.game.get_payoffs()\n",
    "    env.game.players[1 - our_id].hand = orig_hand\n",
    "    return payoffs[our_id]\n",
    "\n",
    "\n",
    "def best_response_value(env, agent, our_id, num_sims, reach_probs):\n",
    "    if env.is_over():\n",
    "        # Fix reach probabilities by removing impossible entries,\n",
    "        # in which our card is the same as the opponent's or the\n",
    "        # public one.\n",
    "        info = env.get_perfect_information()\n",
    "        our_hand = info[\"hand_cards\"][our_id]\n",
    "        our_hand_idx = HAND2IDX[our_hand]\n",
    "        reach_probs[our_hand_idx] = 0.0\n",
    "        ri = np.array(reach_probs)\n",
    "        if info[\"public_card\"]:\n",
    "            public_card = info[\"public_card\"]\n",
    "            public_card_idx = HAND2IDX[public_card]\n",
    "            reach_probs[public_card_idx] = 0.0\n",
    "\n",
    "        if np.sum(reach_probs) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        reach_probs /= np.sum(reach_probs)\n",
    "        \n",
    "        # Compute expected value.\n",
    "        ev = 0.0\n",
    "        for hand, reach_prob in zip(HANDS, reach_probs):\n",
    "            ev += reach_prob * get_payoff(env, hand, our_id)\n",
    "        return ev\n",
    "\n",
    "    curr_player = env.get_player_id()\n",
    "    state = env.get_state(curr_player)\n",
    "    legal_actions = state[\"legal_actions\"]\n",
    "\n",
    "    if our_id == curr_player:\n",
    "        values = np.zeros(env.action_num)\n",
    "        for a in legal_actions:\n",
    "            # First, we check if this action runs into a chance node.\n",
    "            round_before = env.get_perfect_information()[\"current_round\"]\n",
    "            env.step(a)\n",
    "            round_after = env.get_perfect_information()[\"current_round\"]\n",
    "            is_chance_node = round_before != round_after and round_after == 1\n",
    "            env.step_back()\n",
    "\n",
    "            # Then, we compute the expected value of our action.\n",
    "            action_evs = []\n",
    "            for _ in range(num_sims if is_chance_node else 1):\n",
    "                env.step(a)\n",
    "                v = best_response_value(env, agent, our_id, num_sims, reach_probs)\n",
    "                action_evs.append(v)\n",
    "                env.step_back()\n",
    "            values[a] = np.mean(action_evs)\n",
    "            return np.max(values)\n",
    "    else:\n",
    "        # Compute probability of taking action with each holding in the\n",
    "        # opponent's infoset.\n",
    "        probs = np.zeros((6, 4))\n",
    "        for i, hand in enumerate(HANDS):\n",
    "            alt_state = deepcopy(state)\n",
    "            alt_state[\"raw_obs\"][\"hand\"] = hand\n",
    "            _, prob = agent.eval_step(alt_state)\n",
    "            probs[i] = prob\n",
    "\n",
    "        # Recursively compute expected value of state for player with our_id.\n",
    "        values = np.zeros(env.action_num)\n",
    "        for a in legal_actions:\n",
    "            # First, we check if this action runs into a chance node.\n",
    "            round_before = env.get_perfect_information()[\"current_round\"]\n",
    "            env.step(a)\n",
    "            round_after = env.get_perfect_information()[\"current_round\"]\n",
    "            is_chance_node = round_before != round_after and round_after == 1\n",
    "            env.step_back()\n",
    "\n",
    "            # Then, we compute the expected value of our action.\n",
    "            action_evs = []\n",
    "            for _ in range(num_sims if is_chance_node else 1):\n",
    "                env.step(a)\n",
    "                v = best_response_value(env, agent, our_id, num_sims, (probs.T)[a] * reach_probs)\n",
    "                action_evs.append(v)\n",
    "                env.step_back()\n",
    "            values[a] = np.mean(action_evs)\n",
    "        _, p = agent.eval_step(state)\n",
    "        ev = sum(p[a] * values[a] for a in legal_actions)\n",
    "        return ev\n",
    "        \n",
    "\n",
    "def exploitability(env, agent, our_id=0, num_sims=100):\n",
    "    values = []\n",
    "    for _ in range(num_sims):\n",
    "        env.reset()\n",
    "        v = best_response_value(env, agent, our_id, num_sims, reach_probs=np.ones(6))\n",
    "        values.append(v)\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "rIb6643lFDQr"
   },
   "outputs": [],
   "source": [
    "def make_infoset_string(state):\n",
    "    private_card = state[\"raw_obs\"][\"hand\"]\n",
    "    public_card = state[\"raw_obs\"][\"public_card\"] or \"Unknown\"\n",
    "    action_history = \":\".join([record[1] for record in state[\"action_record\"]])\n",
    "    return f\"{private_card}|{public_card}|{action_history}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "1jA1wnYRFDQt"
   },
   "outputs": [],
   "source": [
    "class MCCFR_ES_Agent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.regrets = {}\n",
    "        self.avg_strategy = {}\n",
    "        self.strategy = {}\n",
    "        self.use_raw = False\n",
    "        self.iteration = 0\n",
    "        self.next_traverser = 0\n",
    "    \n",
    "    def regret_matching(self, infoset, legal_actions):\n",
    "        if infoset not in self.regrets or np.all(self.regrets[infoset] <= 0):\n",
    "            action_probs = np.zeros(self.env.action_num)\n",
    "            action_probs[legal_actions] = 1.0 / len(legal_actions)\n",
    "            return action_probs\n",
    "        pos_regrets = np.maximum(self.regrets[infoset], 0)\n",
    "        return pos_regrets / np.sum(pos_regrets)\n",
    "\n",
    "    def train_step(self):\n",
    "        self.iteration += 1\n",
    "        self.env.reset()\n",
    "        self.walk_tree()\n",
    "        self.next_traverser = 1 - self.next_traverser\n",
    "        for infoset in self.avg_strategy:\n",
    "            self.strategy[infoset] = self.avg_strategy[infoset] / np.sum(self.avg_strategy[infoset])\n",
    "        for infoset in self.regrets:\n",
    "            self.regrets[infoset] = np.maximum(self.regrets[infoset], 0)\n",
    "    \n",
    "    def walk_tree(self):\n",
    "        player = self.next_traverser\n",
    "        if self.env.is_over():\n",
    "            payoffs = self.env.get_payoffs()\n",
    "            return payoffs[player]\n",
    "\n",
    "        # Get state information.\n",
    "        curr_player = self.env.get_player_id()\n",
    "        state = self.env.get_state(curr_player)\n",
    "        infoset = make_infoset_string(state)\n",
    "        legal_actions = state[\"legal_actions\"]\n",
    "\n",
    "        # Compute action probabilities proportional to positive regrets.\n",
    "        action_probs = self.regret_matching(infoset, legal_actions)\n",
    "\n",
    "        assert np.isclose(np.sum(action_probs), 1.0), (\n",
    "            f\"Sum of action probs must be approx. 1 but is {np.sum(action_probs)}. \"\n",
    "            f\"Regrets: {self.regrets[infoset]}.\"\n",
    "        )\n",
    "        illegal_actions = ~np.in1d(range(self.env.action_num), legal_actions)\n",
    "        assert np.all(action_probs[illegal_actions] == 0), (\n",
    "            f\"Probability of illegal actions must be 0. \"\n",
    "            f\"Legal actions: {legal_actions}. \"\n",
    "            f\"Action probs: {action_probs}.\"\n",
    "        )\n",
    "\n",
    "      \n",
    "        if curr_player == player:\n",
    "            if infoset not in self.regrets:\n",
    "                self.regrets[infoset] = np.zeros(self.env.action_num)\n",
    "\n",
    "            # Compute action values.\n",
    "            action_values = np.zeros(self.env.action_num)\n",
    "            for a in legal_actions:\n",
    "                self.env.step(a)\n",
    "                action_values[a] = self.walk_tree()\n",
    "                self.env.step_back()\n",
    "            expected_values = np.dot(action_values, action_probs)\n",
    "            # Update regrets.\n",
    "            for a in legal_actions:\n",
    "                self.regrets[infoset][a] += action_values[a] - expected_values\n",
    "\n",
    "            return expected_values\n",
    "        else:\n",
    "            # Sample random action.\n",
    "            action = np.random.choice(range(self.env.action_num), p=action_probs)\n",
    "            if infoset not in self.avg_strategy:\n",
    "                self.avg_strategy[infoset] = np.zeros(env.action_num)\n",
    "\n",
    "            # Update average strategy.\n",
    "            self.avg_strategy[infoset][action] += self.iteration\n",
    "\n",
    "            # Recurse.\n",
    "            self.env.step(action)\n",
    "            value = self.walk_tree()\n",
    "            self.env.step_back()\n",
    "            return value\n",
    "\n",
    "    def eval_step(self, state):\n",
    "        infoset = make_infoset_string(state)\n",
    "        if infoset not in self.strategy:\n",
    "            legal_actions = state[\"legal_actions\"]\n",
    "            action_probs = np.zeros(env.action_num)\n",
    "            action_probs[legal_actions] = 1.0\n",
    "            action_probs /= len(legal_actions)\n",
    "        else:\n",
    "            action_probs = self.strategy[infoset]\n",
    "        action = np.random.choice(range(self.env.action_num), p=action_probs)\n",
    "        return action, action_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "37d5e4a75aa0401d843c8d4913380a3e",
      "8bffac528e414a9c92e93fd0f012158a",
      "abb1f8576e6b4718a602842ed64ce326",
      "06e7fcc6ab284bc1bbcc81daa3a8873f",
      "97bf75ff38b1474db74f861a0e5c3e3f",
      "324e3b7fa70b48fbb27d35c9420bfecc",
      "08823d76f6a74a339c7084bfb09f4e28",
      "48e45878f51a4b11adeedb53d4218f49"
     ]
    },
    "id": "VFKq1zeSFDQu",
    "outputId": "71b4660c-8596-4c07-c68a-6901351ac318",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/rlcard/models/pretrained_models.py:23: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/rlcard/agents/nfsp_agent.py:114: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/rlcard/agents/dqn_agent.py:256: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/rlcard/agents/dqn_agent.py:267: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/normalization.py:327: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/rlcard/agents/dqn_agent.py:280: The name tf.squared_difference is deprecated. Please use tf.math.squared_difference instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/rlcard/agents/dqn_agent.py:242: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/rlcard/agents/dqn_agent.py:242: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/rlcard/agents/dqn_agent.py:242: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/rlcard/agents/dqn_agent.py:244: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/rlcard/agents/dqn_agent.py:247: get_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.get_global_step\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/rlcard/models/pretrained_models.py:40: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from /usr/local/lib/python3.6/dist-packages/rlcard/models/pretrained/leduc_holdem_nfsp/model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37d5e4a75aa0401d843c8d4913380a3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=35001.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t vs random: 0.086 \t vs nfsp: -0.8299 \t vs cfr: -0.77835 \t exp: 1.2253977777777778 +- 0.3952589867047789\n",
      "Epoch: 1000 \t vs random: 0.65655 \t vs nfsp: -0.00955 \t vs cfr: -0.3636 \t exp: 1.1610551453272466 +- 0.34228989432756207\n",
      "Epoch: 2000 \t vs random: 0.66165 \t vs nfsp: 0.1459 \t vs cfr: -0.19165 \t exp: 1.1459003108747867 +- 0.381223311919164\n",
      "Epoch: 3000 \t vs random: 0.7485 \t vs nfsp: 0.2514 \t vs cfr: -0.1997 \t exp: 1.2053283268150738 +- 0.43425185616017964\n",
      "Epoch: 4000 \t vs random: 0.70655 \t vs nfsp: 0.2531 \t vs cfr: -0.18545 \t exp: 1.1319553485220946 +- 0.3424934991749219\n",
      "Epoch: 5000 \t vs random: 0.72105 \t vs nfsp: 0.2338 \t vs cfr: -0.10935 \t exp: 1.0642191950975637 +- 0.3232272052002895\n",
      "Epoch: 6000 \t vs random: 0.73585 \t vs nfsp: 0.24065 \t vs cfr: -0.11555 \t exp: 1.1037486457505787 +- 0.30006763356833477\n",
      "Epoch: 7000 \t vs random: 0.79125 \t vs nfsp: 0.2962 \t vs cfr: -0.13295 \t exp: 0.9947704094969805 +- 0.32474153681720946\n",
      "Epoch: 8000 \t vs random: 0.73275 \t vs nfsp: 0.2464 \t vs cfr: -0.08725 \t exp: 0.9704753302660822 +- 0.3432994299835305\n",
      "Epoch: 9000 \t vs random: 0.7774 \t vs nfsp: 0.34375 \t vs cfr: -0.0871 \t exp: 1.0297423508360388 +- 0.3159744969607855\n",
      "Epoch: 10000 \t vs random: 0.7458 \t vs nfsp: 0.2713 \t vs cfr: 0.01595 \t exp: 0.9455637734486939 +- 0.26888612104441784\n",
      "Epoch: 11000 \t vs random: 0.7662 \t vs nfsp: 0.32215 \t vs cfr: -0.09005 \t exp: 0.9541820634056092 +- 0.32512726243786333\n",
      "Epoch: 12000 \t vs random: 0.8056 \t vs nfsp: 0.3416 \t vs cfr: -0.00275 \t exp: 1.0623606863392765 +- 0.2926037327890837\n",
      "Epoch: 13000 \t vs random: 0.7864 \t vs nfsp: 0.3887 \t vs cfr: -0.02265 \t exp: 0.9825133895712603 +- 0.32851844212914494\n",
      "Epoch: 14000 \t vs random: 0.80815 \t vs nfsp: 0.2947 \t vs cfr: -0.0074 \t exp: 0.9976939487214541 +- 0.3276656255797863\n",
      "Epoch: 15000 \t vs random: 0.8012 \t vs nfsp: 0.3332 \t vs cfr: -0.0108 \t exp: 0.9901553716504496 +- 0.3249409905795452\n",
      "Epoch: 16000 \t vs random: 0.80115 \t vs nfsp: 0.34685 \t vs cfr: 0.00035 \t exp: 1.0124716950948516 +- 0.3943877234963953\n",
      "Epoch: 17000 \t vs random: 0.80235 \t vs nfsp: 0.3443 \t vs cfr: -0.0084 \t exp: 0.9471151058480902 +- 0.29530574665394727\n",
      "Epoch: 18000 \t vs random: 0.7958 \t vs nfsp: 0.3705 \t vs cfr: -0.0096 \t exp: 1.0297605046014602 +- 0.29528768943441985\n",
      "Epoch: 19000 \t vs random: 0.80155 \t vs nfsp: 0.40565 \t vs cfr: -0.0235 \t exp: 0.9711903118002396 +- 0.2717594096982845\n",
      "Epoch: 20000 \t vs random: 0.80055 \t vs nfsp: 0.3701 \t vs cfr: 0.0219 \t exp: 1.0294181130060522 +- 0.3337558140044359\n",
      "Epoch: 21000 \t vs random: 0.8094 \t vs nfsp: 0.3519 \t vs cfr: 0.0244 \t exp: 1.0313543624745223 +- 0.33388711673855714\n",
      "Epoch: 22000 \t vs random: 0.82895 \t vs nfsp: 0.36015 \t vs cfr: -0.03985 \t exp: 0.9759512364987389 +- 0.3078087107389749\n",
      "Epoch: 23000 \t vs random: 0.8109 \t vs nfsp: 0.37405 \t vs cfr: -0.00065 \t exp: 0.9665724387440038 +- 0.30922332203184716\n",
      "Epoch: 24000 \t vs random: 0.80755 \t vs nfsp: 0.3913 \t vs cfr: 0.0137 \t exp: 1.0527607382439372 +- 0.328705744702001\n",
      "Epoch: 25000 \t vs random: 0.79165 \t vs nfsp: 0.40365 \t vs cfr: -0.01705 \t exp: 1.0240667738886886 +- 0.3338700295850688\n",
      "Epoch: 26000 \t vs random: 0.84515 \t vs nfsp: 0.41165 \t vs cfr: 0.03505 \t exp: 1.0567615281508207 +- 0.3648172526452389\n",
      "Epoch: 27000 \t vs random: 0.844 \t vs nfsp: 0.38475 \t vs cfr: 0.04675 \t exp: 0.9828630827267891 +- 0.33479460611730644\n",
      "Epoch: 28000 \t vs random: 0.83475 \t vs nfsp: 0.33965 \t vs cfr: 0.0502 \t exp: 0.96675706420205 +- 0.3155393759387548\n",
      "Epoch: 29000 \t vs random: 0.7981 \t vs nfsp: 0.41615 \t vs cfr: 0.05075 \t exp: 1.0185151245252149 +- 0.36595668224804967\n",
      "Epoch: 30000 \t vs random: 0.8023 \t vs nfsp: 0.4497 \t vs cfr: 0.0458 \t exp: 1.0050010663117752 +- 0.3480294816871173\n",
      "Epoch: 31000 \t vs random: 0.80995 \t vs nfsp: 0.32975 \t vs cfr: 0.0435 \t exp: 1.0557705666452462 +- 0.3351668189299973\n",
      "Epoch: 32000 \t vs random: 0.83905 \t vs nfsp: 0.3943 \t vs cfr: 0.00785 \t exp: 0.9762476393448574 +- 0.3487788547355338\n",
      "Epoch: 33000 \t vs random: 0.8022 \t vs nfsp: 0.40965 \t vs cfr: 0.10615 \t exp: 1.0034287970190343 +- 0.3174382266667121\n",
      "Epoch: 34000 \t vs random: 0.8057 \t vs nfsp: 0.33045 \t vs cfr: 0.0174 \t exp: 1.0586279487517394 +- 0.3651993848609163\n",
      "Epoch: 35000 \t vs random: 0.8672 \t vs nfsp: 0.3421 \t vs cfr: 0.07165 \t exp: 1.0007770747321982 +- 0.3199014670402766\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 35000\n",
    "EVAL_INTERVAL = 1000\n",
    "EVAL_ITERS = 10000\n",
    "SAVE_DIR = \"./agents\"\n",
    "SAVE_FILE = \"mccfr_model.pkl\"\n",
    "RESULTS_DIR = './results/'\n",
    "if not os.path.isdir(RESULTS_DIR):\n",
    "    os.mkdir(RESULTS_DIR)\n",
    "\n",
    "SAVE_PATH = os.path.join(SAVE_DIR, SAVE_FILE)\n",
    "\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "env = LeducholdemEnv(config={\"allow_step_back\": True, \"allow_raw_data\": True, \"record_action\": True})\n",
    "eval_env_1 = LeducholdemEnv(config={\"allow_step_back\": True, \"allow_raw_data\": True, \"record_action\": True})\n",
    "eval_env_2 = LeducholdemEnv(config={\"allow_step_back\": False, \"allow_raw_data\": True, \"record_action\": True})\n",
    "eval_env_3 = LeducholdemEnv(config={\"allow_step_back\": False, \"allow_raw_data\": True, \"record_action\": True})\n",
    "eval_env_4 = LeducholdemEnv(config={\"allow_step_back\": False, \"allow_raw_data\": True, \"record_action\": True})\n",
    "\n",
    "agent = MCCFR_ES_Agent(env)\n",
    "random_agent = RandomAgent(4)\n",
    "nfsp_agent = models.load(\"leduc-holdem-nfsp\").agents[0]\n",
    "cfr_agent = models.load(\"leduc-holdem-cfr\").agents[0]\n",
    "\n",
    "eval_env_2.set_agents([agent, random_agent])\n",
    "eval_env_3.set_agents([agent, nfsp_agent])\n",
    "eval_env_4.set_agents([agent, cfr_agent])\n",
    "\n",
    "random_rewards = []\n",
    "nfsp_rewards = []\n",
    "cfr_rewards = []\n",
    "history = []\n",
    "for epoch in trange(0, NUM_EPOCHS+1):\n",
    "    agent.train_step()\n",
    "    if epoch % EVAL_INTERVAL == 0:\n",
    "        expl = exploitability(eval_env_1, agent, our_id=0, num_sims=50)\n",
    "\n",
    "        vs_random = tournament(eval_env_2, EVAL_ITERS)[0]\n",
    "        vs_nfsp = tournament(eval_env_3, EVAL_ITERS)[0]\n",
    "        vs_cfr = tournament(eval_env_4, EVAL_ITERS)[0]\n",
    "        \n",
    "        random_rewards.append(vs_random)\n",
    "        nfsp_rewards.append(nfsp_rewards)\n",
    "        cfr_rewards.append(vs_cfr)\n",
    "        history.append((vs_random, vs_nfsp, vs_cfr, np.mean(expl), np.std(expl)))\n",
    "        np.save(RESULTS_DIR + 'mccfr', history)\n",
    "        print(f\"Epoch: {epoch} \\t vs random: {vs_random} \\t vs nfsp: {vs_nfsp} \\t vs cfr: {vs_cfr} \\t exp: {np.mean(expl)} +- {np.std(expl)}\")\n",
    "        pickle.dump(agent, open(SAVE_PATH, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "id": "ZnK6rRedvfNt",
    "outputId": "81e6c801-312a-4f01-a756-051dd4c5bf21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: content/agents/ (stored 0%)\n",
      "  adding: content/agents/mccfr_model.pkl (deflated 66%)\n",
      "  adding: content/results/ (stored 0%)\n",
      "  adding: content/results/mccfr.npy (deflated 11%)\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_c66d7563-3c84-4529-b3e4-0a1bf2a99f6e\", \"agents.zip\", 110324)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_b232fedd-2a5e-4437-8acc-6d9ae7344787\", \"results.zip\", 1756)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from google.colab import files\n",
    "!zip -r /content/agents.zip /content/agents\n",
    "!zip -r /content/results.zip /content/results\n",
    "files.download(\"/content/agents.zip\")\n",
    "files.download(\"/content/results.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OIknPKKivmdn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ucLnVdSQvmf4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jgqyuogWvmiQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ogJAwVF5vmkg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L7PmpuTzvmmr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PTd9Wepavmos"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xtZGAdYzFDQx",
    "outputId": "b895ae1a-50db-43ef-e0c6-c97ef5f72d9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Leduc Hold'em pre-trained model\n",
      ">> Start a new game\n",
      "\n",
      "=============== Community Card ===============\n",
      "┌─────────┐\n",
      "│░░░░░░░░░│\n",
      "│░░░░░░░░░│\n",
      "│░░░░░░░░░│\n",
      "│░░░░░░░░░│\n",
      "│░░░░░░░░░│\n",
      "│░░░░░░░░░│\n",
      "│░░░░░░░░░│\n",
      "└─────────┘\n",
      "===============   Your Hand    ===============\n",
      "┌─────────┐\n",
      "│J        │\n",
      "│         │\n",
      "│         │\n",
      "│    ♠    │\n",
      "│         │\n",
      "│         │\n",
      "│        J│\n",
      "└─────────┘\n",
      "===============     Chips      ===============\n",
      "Yours:   +\n",
      "Agent 1: ++\n",
      "=========== Actions You Can Choose ===========\n",
      "0: call, 1: raise, 2: fold\n",
      "\n",
      ">> You choose action (integer): 0\n",
      ">> Player 1 chooses check\n",
      "\n",
      "=============== Community Card ===============\n",
      "┌─────────┐\n",
      "│Q        │\n",
      "│         │\n",
      "│         │\n",
      "│    ♠    │\n",
      "│         │\n",
      "│         │\n",
      "│        Q│\n",
      "└─────────┘\n",
      "===============   Your Hand    ===============\n",
      "┌─────────┐\n",
      "│J        │\n",
      "│         │\n",
      "│         │\n",
      "│    ♠    │\n",
      "│         │\n",
      "│         │\n",
      "│        J│\n",
      "└─────────┘\n",
      "===============     Chips      ===============\n",
      "Yours:   ++\n",
      "Agent 1: ++\n",
      "=========== Actions You Can Choose ===========\n",
      "0: raise, 1: fold, 2: check\n",
      "\n",
      ">> You choose action (integer): 2\n",
      ">> Player 1 chooses raise\n",
      "\n",
      "=============== Community Card ===============\n",
      "┌─────────┐\n",
      "│Q        │\n",
      "│         │\n",
      "│         │\n",
      "│    ♠    │\n",
      "│         │\n",
      "│         │\n",
      "│        Q│\n",
      "└─────────┘\n",
      "===============   Your Hand    ===============\n",
      "┌─────────┐\n",
      "│J        │\n",
      "│         │\n",
      "│         │\n",
      "│    ♠    │\n",
      "│         │\n",
      "│         │\n",
      "│        J│\n",
      "└─────────┘\n",
      "===============     Chips      ===============\n",
      "Yours:   ++\n",
      "Agent 1: ++++++\n",
      "=========== Actions You Can Choose ===========\n",
      "0: call, 1: raise, 2: fold\n",
      "\n",
      ">> You choose action (integer): 2\n",
      ">> Player 0 chooses fold\n",
      "===============     CFR Agent    ===============\n",
      "┌─────────┐\n",
      "│J        │\n",
      "│         │\n",
      "│         │\n",
      "│    ♥    │\n",
      "│         │\n",
      "│         │\n",
      "│        J│\n",
      "└─────────┘\n",
      "===============     Result     ===============\n",
      "You lose 1.0 chips!\n",
      "\n",
      "Press any key to continue...\n",
      ">> Start a new game\n",
      "\n",
      "=============== Community Card ===============\n",
      "┌─────────┐\n",
      "│░░░░░░░░░│\n",
      "│░░░░░░░░░│\n",
      "│░░░░░░░░░│\n",
      "│░░░░░░░░░│\n",
      "│░░░░░░░░░│\n",
      "│░░░░░░░░░│\n",
      "│░░░░░░░░░│\n",
      "└─────────┘\n",
      "===============   Your Hand    ===============\n",
      "┌─────────┐\n",
      "│J        │\n",
      "│         │\n",
      "│         │\n",
      "│    ♠    │\n",
      "│         │\n",
      "│         │\n",
      "│        J│\n",
      "└─────────┘\n",
      "===============     Chips      ===============\n",
      "Yours:   +\n",
      "Agent 1: ++\n",
      "=========== Actions You Can Choose ===========\n",
      "0: call, 1: raise, 2: fold\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-149-50fa841efc56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\">> Start a new game\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mtrajectories\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpayoffs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;31m# If the human does not take the final action, we need to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# print other players action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.9/lib/python3.7/site-packages/rlcard/envs/env.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, is_training)\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0;31m# Agent plays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m                 \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mplayer_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mplayer_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.9/lib/python3.7/site-packages/rlcard/agents/leduc_holdem_human_agent.py\u001b[0m in \u001b[0;36meval_step\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mprobs\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0maction\u001b[0m \u001b[0mprobabilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         '''\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_print_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_record\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.9/lib/python3.7/site-packages/rlcard/agents/leduc_holdem_human_agent.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     26\u001b[0m         '''\n\u001b[1;32m     27\u001b[0m         \u001b[0m_print_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'raw_obs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'action_record'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'>> You choose action (integer): '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0maction\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'legal_actions'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Action illegel...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.9/lib/python3.7/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m         )\n\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.9/lib/python3.7/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "import rlcard\n",
    "from rlcard import models\n",
    "from rlcard.agents import LeducholdemHumanAgent as HumanAgent\n",
    "from rlcard.utils import print_card\n",
    "\n",
    "# Make environment\n",
    "# Set 'record_action' to True because we need it to print results\n",
    "env = rlcard.make('leduc-holdem', config={'record_action': True})\n",
    "human_agent = HumanAgent(env.action_num)\n",
    "env.set_agents([human_agent, agent])\n",
    "\n",
    "print(\">> Leduc Hold'em pre-trained model\")\n",
    "\n",
    "while True:\n",
    "    print(\">> Start a new game\")\n",
    "\n",
    "    trajectories, payoffs = env.run(is_training=False)\n",
    "    # If the human does not take the final action, we need to\n",
    "    # print other players action\n",
    "    final_state = trajectories[0][-1][-2]\n",
    "    action_record = final_state['action_record']\n",
    "    state = final_state['raw_obs']\n",
    "    _action_list = []\n",
    "    for i in range(1, len(action_record)+1):\n",
    "        if action_record[-i][0] == state['current_player']:\n",
    "            break\n",
    "        _action_list.insert(0, action_record[-i])\n",
    "    for pair in _action_list:\n",
    "        print('>> Player', pair[0], 'chooses', pair[1])\n",
    "\n",
    "    # Let's take a look at what the agent card is\n",
    "    print('===============     CFR Agent    ===============')\n",
    "    print_card(env.get_perfect_information()['hand_cards'][1])\n",
    "\n",
    "    print('===============     Result     ===============')\n",
    "    if payoffs[0] > 0:\n",
    "        print('You win {} chips!'.format(payoffs[0]))\n",
    "    elif payoffs[0] == 0:\n",
    "        print('It is a tie.')\n",
    "    else:\n",
    "        print('You lose {} chips!'.format(-payoffs[0]))\n",
    "    print('')\n",
    "\n",
    "    input(\"Press any key to continue...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T_qxmwDxFDQz"
   },
   "outputs": [],
   "source": [
    "eval_env = rlcard.make(\"leduc-holdem\", config={\"seed\": SEED, \"allow_step_back\": True})\n",
    "eval_env.set_agents([models.load(\"leduc-holdem-cfr\").agents[0], models.load(\"leduc-holdem-nfsp\").agents[0]])\n",
    "print(tournament(eval_env, 100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A8CmAkGKFDQ0"
   },
   "outputs": [],
   "source": [
    "class MCCFRAgent:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def train(self, num_iter):\n",
    "        pass\n",
    "    \n",
    "    def eval_step(self, state):\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "MCCFR.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "06e7fcc6ab284bc1bbcc81daa3a8873f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_48e45878f51a4b11adeedb53d4218f49",
      "placeholder": "​",
      "style": "IPY_MODEL_08823d76f6a74a339c7084bfb09f4e28",
      "value": " 35001/35001 [32:14&lt;00:00, 18.09it/s]"
     }
    },
    "08823d76f6a74a339c7084bfb09f4e28": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "324e3b7fa70b48fbb27d35c9420bfecc": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "37d5e4a75aa0401d843c8d4913380a3e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_abb1f8576e6b4718a602842ed64ce326",
       "IPY_MODEL_06e7fcc6ab284bc1bbcc81daa3a8873f"
      ],
      "layout": "IPY_MODEL_8bffac528e414a9c92e93fd0f012158a"
     }
    },
    "48e45878f51a4b11adeedb53d4218f49": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8bffac528e414a9c92e93fd0f012158a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "97bf75ff38b1474db74f861a0e5c3e3f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "abb1f8576e6b4718a602842ed64ce326": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_324e3b7fa70b48fbb27d35c9420bfecc",
      "max": 35001,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_97bf75ff38b1474db74f861a0e5c3e3f",
      "value": 35001
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

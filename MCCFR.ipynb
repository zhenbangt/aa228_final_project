{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "MCCFR.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4e770eb002284478bd382d6647d4e02d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_86887ef04e7843f89d1232f058a1c44e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c9b1b09563314b40af056802de1a0727",
              "IPY_MODEL_ffb4ee64d9fd44a0a8058d0f044f1339"
            ]
          }
        },
        "86887ef04e7843f89d1232f058a1c44e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c9b1b09563314b40af056802de1a0727": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9cb5f676791b4670a5389f36a51ea68b",
            "_dom_classes": [],
            "description": "  2%",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 1000000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 19980,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_30d6880aa00e4c5b9b5309e21f6f5898"
          }
        },
        "ffb4ee64d9fd44a0a8058d0f044f1339": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_853ea831d9aa400193d555c589f2eebc",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 19980/1000000 [02:06&lt;1:20:10, 203.74it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a9f76b3949164b2cb8c5a8141dd1e8d8"
          }
        },
        "9cb5f676791b4670a5389f36a51ea68b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "30d6880aa00e4c5b9b5309e21f6f5898": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "853ea831d9aa400193d555c589f2eebc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a9f76b3949164b2cb8c5a8141dd1e8d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhenbangt/aa228_final_project/blob/main/MCCFR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DqVVm7cFDQh"
      },
      "source": [
        "# Solving Poker using MCCFR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOwrJYpkFw-z",
        "outputId": "1f943e45-fc2e-48e6-f313-59439ff125a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env.py\tsample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUr5gQovFDQh"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0FcxH2fF6Tv",
        "outputId": "0b402a4b-dcd8-414a-ed63-5e71b1ba9886",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install -q rlcard\n",
        "!pip install -q rlcard[tensorflow]\n",
        "!pip install -q tqdm"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      Successfully uninstalled tensorflow-2.3.0\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.4 tensorflow-estimator-1.15.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "Sj4k3yfuFDQk"
      },
      "source": [
        "import math\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "from copy import copy, deepcopy\n",
        "\n",
        "import numpy as np\n",
        "import rlcard\n",
        "from pprint import pprint\n",
        "from rlcard import models\n",
        "from rlcard.agents import RandomAgent\n",
        "from rlcard.agents.best_response_agent import BRAgent\n",
        "from rlcard.core import Card\n",
        "from rlcard.utils import tournament\n",
        "from tqdm.notebook import tqdm, trange\n",
        "\n",
        "from env import LeducholdemEnv"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_GSYHYCFDQn"
      },
      "source": [
        "HANDS = [\"SJ\", \"SQ\", \"SK\", \"HJ\", \"HQ\", \"HK\"]\n",
        "HAND2IDX = {hand: i for i, hand in enumerate(HANDS)}\n",
        "\n",
        "\n",
        "def get_payoff(env, hand, our_id):\n",
        "    orig_hand = copy(env.game.players[1 - our_id].hand)\n",
        "    env.game.players[1 - our_id].hand = Card(suit=hand[0], rank=hand[1])\n",
        "    payoffs = env.game.get_payoffs()\n",
        "    env.game.players[1 - our_id].hand = orig_hand\n",
        "    return payoffs[our_id]\n",
        "\n",
        "\n",
        "def best_response_value(env, agent, our_id, num_sims, reach_probs):\n",
        "    if env.is_over():\n",
        "        # Fix reach probabilities by removing impossible entries,\n",
        "        # in which our card is the same as the opponent's or the\n",
        "        # public one.\n",
        "        info = env.get_perfect_information()\n",
        "        our_hand = info[\"hand_cards\"][our_id]\n",
        "        our_hand_idx = HAND2IDX[our_hand]\n",
        "        reach_probs[our_hand_idx] = 0.0\n",
        "        ri = np.array(reach_probs)\n",
        "        if info[\"public_card\"]:\n",
        "            public_card = info[\"public_card\"]\n",
        "            public_card_idx = HAND2IDX[public_card]\n",
        "            reach_probs[public_card_idx] = 0.0\n",
        "\n",
        "        if np.sum(reach_probs) == 0:\n",
        "            return 0.0\n",
        "        \n",
        "        reach_probs /= np.sum(reach_probs)\n",
        "        \n",
        "        # Compute expected value.\n",
        "        ev = 0.0\n",
        "        for hand, reach_prob in zip(HANDS, reach_probs):\n",
        "            ev += reach_prob * get_payoff(env, hand, our_id)\n",
        "        return ev\n",
        "\n",
        "    curr_player = env.get_player_id()\n",
        "    state = env.get_state(curr_player)\n",
        "    legal_actions = state[\"legal_actions\"]\n",
        "\n",
        "    if our_id == curr_player:\n",
        "        values = np.zeros(env.action_num)\n",
        "        for a in legal_actions:\n",
        "            # First, we check if this action runs into a chance node.\n",
        "            round_before = env.get_perfect_information()[\"current_round\"]\n",
        "            env.step(a)\n",
        "            round_after = env.get_perfect_information()[\"current_round\"]\n",
        "            is_chance_node = round_before != round_after and round_after == 1\n",
        "            env.step_back()\n",
        "\n",
        "            # Then, we compute the expected value of our action.\n",
        "            action_evs = []\n",
        "            for _ in range(num_sims if is_chance_node else 1):\n",
        "                env.step(a)\n",
        "                v = best_response_value(env, agent, our_id, num_sims, reach_probs)\n",
        "                action_evs.append(v)\n",
        "                env.step_back()\n",
        "            values[a] = np.mean(action_evs)\n",
        "            return np.max(values)\n",
        "    else:\n",
        "        # Compute probability of taking action with each holding in the\n",
        "        # opponent's infoset.\n",
        "        probs = np.zeros((6, 4))\n",
        "        for i, hand in enumerate(HANDS):\n",
        "            alt_state = deepcopy(state)\n",
        "            alt_state[\"raw_obs\"][\"hand\"] = hand\n",
        "            _, prob = agent.eval_step(alt_state)\n",
        "            probs[i] = prob\n",
        "\n",
        "        # Recursively compute expected value of state for player with our_id.\n",
        "        values = np.zeros(env.action_num)\n",
        "        for a in legal_actions:\n",
        "            # First, we check if this action runs into a chance node.\n",
        "            round_before = env.get_perfect_information()[\"current_round\"]\n",
        "            env.step(a)\n",
        "            round_after = env.get_perfect_information()[\"current_round\"]\n",
        "            is_chance_node = round_before != round_after and round_after == 1\n",
        "            env.step_back()\n",
        "\n",
        "            # Then, we compute the expected value of our action.\n",
        "            action_evs = []\n",
        "            for _ in range(num_sims if is_chance_node else 1):\n",
        "                env.step(a)\n",
        "                v = best_response_value(env, agent, our_id, num_sims, (probs.T)[a] * reach_probs)\n",
        "                action_evs.append(v)\n",
        "                env.step_back()\n",
        "            values[a] = np.mean(action_evs)\n",
        "        _, p = agent.eval_step(state)\n",
        "        ev = sum(p[a] * values[a] for a in legal_actions)\n",
        "        return ev\n",
        "        \n",
        "\n",
        "def exploitability(env, agent, our_id=0, num_sims=100):\n",
        "    values = []\n",
        "    for _ in range(num_sims):\n",
        "        env.reset()\n",
        "        v = best_response_value(env, agent, our_id, num_sims, reach_probs=np.ones(6))\n",
        "        values.append(v)\n",
        "    return values"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCUqivH4FDQp",
        "outputId": "04f92273-1c58-4ffb-9938-6989708cab9a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Test exploitability method.\n",
        "env = LeducholdemEnv(config={\"allow_step_back\": True, \"allow_raw_data\": True, \"record_action\": True})\n",
        "values = exploitability(env, RandomAgent(4), our_id=0, num_sims=25)\n",
        "print(f\"Value: {np.mean(values)} +- {np.std(values)}\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Value: 1.2437185185185184 +- 0.22225870021592747\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIb6643lFDQr"
      },
      "source": [
        "def make_infoset_string(state):\n",
        "    private_card = state[\"raw_obs\"][\"hand\"]\n",
        "    public_card = state[\"raw_obs\"][\"public_card\"] or \"Unknown\"\n",
        "    action_history = \":\".join([record[1] for record in state[\"action_record\"]])\n",
        "    return f\"{private_card}|{public_card}|{action_history}\""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jA1wnYRFDQt"
      },
      "source": [
        "class MCCFR_ES_Agent:\n",
        "    def __init__(self, env, _update_iterations=100):\n",
        "        self.env = env\n",
        "        self.regrets = {}\n",
        "        self.avg_strategy = {}\n",
        "        self.strategy = {}\n",
        "        self.use_raw = False\n",
        "        self.iteration = 0\n",
        "        self.next_traverser = 0\n",
        "        self._update_iterations = _update_iterations\n",
        "    \n",
        "    def regret_matching(self, infoset, legal_actions):\n",
        "        if infoset not in self.regrets or np.all(self.regrets[infoset] <= 0):\n",
        "            action_probs = np.zeros(self.env.action_num)\n",
        "            action_probs[legal_actions] = 1.0 / len(legal_actions)\n",
        "            return action_probs\n",
        "        pos_regrets = np.maximum(self.regrets[infoset], 0)\n",
        "        return pos_regrets / np.sum(pos_regrets)\n",
        "\n",
        "    def update_cumulative_profile(self):\n",
        "        curr_player = self.env.get_player_id()\n",
        "        if self.env.is_over():\n",
        "            return\n",
        "        state = self.env.get_state(curr_player)\n",
        "        infoset = make_infoset_string(state)\n",
        "        legal_actions = state[\"legal_actions\"]\n",
        "\n",
        "        if curr_player != self.next_traverser:\n",
        "            for action in legal_actions:\n",
        "                self.env.step(action)\n",
        "                self.update_cumulative_profile()\n",
        "                self.env.step_back()\n",
        "\n",
        "        elif curr_player==self.next_traverser:\n",
        "            action_probs = self.regret_matching(infoset, legal_actions)\n",
        "            if infoset not in self.avg_strategy:\n",
        "                self.avg_strategy[infoset] = np.zeros(env.action_num)\n",
        "            for action in legal_actions:\n",
        "                self.avg_strategy[infoset][action] += action_probs[action]\n",
        "            action = np.random.choice(range(self.env.action_num), p=action_probs)\n",
        "            self.env.step(action)\n",
        "            self.update_cumulative_profile()\n",
        "            \n",
        "    def train_step(self):\n",
        "        self.iteration += 1\n",
        "        self.env.reset()\n",
        "        # to make it fare also alternate player like purecfr\n",
        "        self.walk_tree()\n",
        "        _player = self.next_traverser\n",
        "\n",
        "        for i in range(2):\n",
        "          if self.iteration % self._update_iterations==0:\n",
        "            self.next_traverser = i\n",
        "            self.env.reset()\n",
        "            self.update_cumulative_profile()\n",
        "\n",
        "        self.next_traverser = _player\n",
        "        self.next_traverser = 1 - self.next_traverser\n",
        "\n",
        "        # self.pure_cfr()\n",
        "        # self.next_traverser = 1 - self.next_traverser\n",
        "        for infoset in self.avg_strategy:\n",
        "            self.strategy[infoset] = self.avg_strategy[infoset] / np.sum(self.avg_strategy[infoset])\n",
        "        for infoset in self.regrets:\n",
        "            self.regrets[infoset] = np.maximum(self.regrets[infoset], 0)\n",
        "    \n",
        "    def walk_tree(self):\n",
        "        player = self.next_traverser\n",
        "        if self.env.is_over():\n",
        "            payoffs = self.env.get_payoffs()\n",
        "            return payoffs[player]\n",
        "\n",
        "        # Get state information.\n",
        "        curr_player = self.env.get_player_id()\n",
        "        state = self.env.get_state(curr_player)\n",
        "        infoset = make_infoset_string(state)\n",
        "        legal_actions = state[\"legal_actions\"]\n",
        "        # Compute action probabilities proportional to positive regrets.\n",
        "        action_probs = self.regret_matching(infoset, legal_actions)\n",
        "        assert np.isclose(np.sum(action_probs), 1.0), (\n",
        "            f\"Sum of action probs must be approx. 1 but is {np.sum(action_probs)}. \"\n",
        "            f\"Regrets: {self.regrets[infoset]}.\"\n",
        "        )\n",
        "        illegal_actions = ~np.in1d(range(self.env.action_num), legal_actions)\n",
        "        assert np.all(action_probs[illegal_actions] == 0), (\n",
        "            f\"Probability of illegal actions must be 0. \"\n",
        "            f\"Legal actions: {legal_actions}. \"\n",
        "            f\"Action probs: {action_probs}.\"\n",
        "        )\n",
        "\n",
        "        if curr_player != player:\n",
        "            # Sample action.\n",
        "            action = np.random.choice(range(self.env.action_num), p=action_probs)\n",
        "            env.step(action)\n",
        "            return self.walk_tree()\n",
        "\n",
        "        else:\n",
        "            if infoset not in self.regrets:\n",
        "                self.regrets[infoset] = np.zeros(self.env.action_num)\n",
        "            # Compute action values.\n",
        "            action_values = np.zeros(self.env.action_num)\n",
        "            for action in legal_actions:\n",
        "                self.env.step(action)\n",
        "                action_values[action] = self.walk_tree()\n",
        "                self.env.step_back()\n",
        "\n",
        "            expected_value = np.sum([action_values[action] * action_probs[action] for action in legal_actions])\n",
        "            # Update regrets.\n",
        "            for action in legal_actions:\n",
        "                self.regrets[infoset][action] += action_values[action] - expected_value\n",
        "            return expected_value\n",
        "\n",
        "    def eval_step(self, state):\n",
        "        infoset = make_infoset_string(state)\n",
        "        if infoset not in self.strategy:\n",
        "            legal_actions = state[\"legal_actions\"]\n",
        "            action_probs = np.zeros(env.action_num)\n",
        "            action_probs[legal_actions] = 1.0\n",
        "            action_probs /= len(legal_actions)\n",
        "        else:\n",
        "            action_probs = self.strategy[infoset]\n",
        "        action = np.random.choice(range(self.env.action_num), p=action_probs)\n",
        "        return action, action_probs"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "VFKq1zeSFDQu",
        "outputId": "bc36b0ce-3abc-40a2-d78e-05e7c8c63e1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443,
          "referenced_widgets": [
            "4e770eb002284478bd382d6647d4e02d",
            "86887ef04e7843f89d1232f058a1c44e",
            "c9b1b09563314b40af056802de1a0727",
            "ffb4ee64d9fd44a0a8058d0f044f1339",
            "9cb5f676791b4670a5389f36a51ea68b",
            "30d6880aa00e4c5b9b5309e21f6f5898",
            "853ea831d9aa400193d555c589f2eebc",
            "a9f76b3949164b2cb8c5a8141dd1e8d8"
          ]
        }
      },
      "source": [
        "NUM_EPOCHS = 25000\n",
        "EVAL_INTERVAL = 1000\n",
        "EVAL_ITERS = 10000\n",
        "SAVE_DIR = \"./agents\"\n",
        "SAVE_FILE = \"cfr_model.pkl\"\n",
        "SAVE_PATH = os.path.join(SAVE_DIR, SAVE_FILE)\n",
        "\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "env = LeducholdemEnv(config={\"allow_step_back\": True, \"allow_raw_data\": True, \"record_action\": True})\n",
        "eval_env_1 = LeducholdemEnv(config={\"allow_step_back\": True, \"allow_raw_data\": True, \"record_action\": True})\n",
        "eval_env_2 = LeducholdemEnv(config={\"allow_step_back\": False, \"allow_raw_data\": True, \"record_action\": True})\n",
        "eval_env_3 = LeducholdemEnv(config={\"allow_step_back\": False, \"allow_raw_data\": True, \"record_action\": True})\n",
        "eval_env_4 = LeducholdemEnv(config={\"allow_step_back\": False, \"allow_raw_data\": True, \"record_action\": True})\n",
        "\n",
        "agent = PureCFRAgent(env)\n",
        "random_agent = RandomAgent(4)\n",
        "nfsp_agent = models.load(\"leduc-holdem-nfsp\").agents[0]\n",
        "cfr_agent = models.load(\"leduc-holdem-cfr\").agents[0]\n",
        "\n",
        "eval_env_2.set_agents([agent, random_agent])\n",
        "eval_env_3.set_agents([agent, nfsp_agent])\n",
        "eval_env_4.set_agents([agent, cfr_agent])\n",
        "\n",
        "random_rewards = []\n",
        "nfsp_rewards = []\n",
        "cfr_rewards = []\n",
        "for epoch in trange(1, NUM_EPOCHS + 1):\n",
        "    agent.train_step()\n",
        "    if epoch % EVAL_INTERVAL == 0:\n",
        "        expl = exploitability(eval_env_1, agent, our_id=0, num_sims=50)\n",
        "\n",
        "        vs_random = tournament(eval_env_2, EVAL_ITERS)[0]\n",
        "        vs_nfsp = tournament(eval_env_3, EVAL_ITERS)[0]\n",
        "        vs_cfr = tournament(eval_env_4, EVAL_ITERS)[0]\n",
        "        \n",
        "        random_rewards.append(vs_random)\n",
        "        nfsp_rewards.append(nfsp_rewards)\n",
        "        cfr_rewards.append(vs_cfr)\n",
        "\n",
        "        print(f\"Epoch: {epoch} \\t vs random: {vs_random} \\t vs nfsp: {vs_nfsp} \\t vs cfr: {vs_cfr} \\t exp: {np.mean(expl)} +- {np.std(expl)}\")\n",
        "        pickle.dump(agent, open(SAVE_PATH, \"wb\"))"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4e770eb002284478bd382d6647d4e02d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1000000.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 10000 \t Avg reward: -0.8551 \t Exploitability: 1.614235793061161 +- 0.2971398891936089\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-85-4aa079b28f84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mEVAL_INTERVAL\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mavg_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtournament\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_env_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEVAL_ITERS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mexpl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexploitability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_env_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mour_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_sims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch: {epoch} \\t Avg reward: {avg_reward} \\t Exploitability: {np.mean(expl)} +- {np.std(expl)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/rlcard/utils/utils.py\u001b[0m in \u001b[0;36mtournament\u001b[0;34m(env, num)\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mcounter\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_payoffs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_payoffs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_p\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_payoffs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/env.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, is_training)\u001b[0m\n\u001b[1;32m    406\u001b[0m             \u001b[0;31m# Agent plays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m                 \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mplayer_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mplayer_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/rlcard/agents/nfsp_agent.py\u001b[0m in \u001b[0;36meval_step\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'obs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0mlegal_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'legal_actions'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_act\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m             \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_illegal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlegal_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/rlcard/agents/nfsp_agent.py\u001b[0m in \u001b[0;36m_act\u001b[0;34m(self, info_state)\u001b[0m\n\u001b[1;32m    240\u001b[0m         action_probs = self._sess.run(\n\u001b[1;32m    241\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_avg_policy_probs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m                 feed_dict={self._info_state_ph: info_state, self.is_train: False})[0]\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maction_probs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtZGAdYzFDQx",
        "outputId": "b895ae1a-50db-43ef-e0c6-c97ef5f72d9b"
      },
      "source": [
        "import rlcard\n",
        "from rlcard import models\n",
        "from rlcard.agents import LeducholdemHumanAgent as HumanAgent\n",
        "from rlcard.utils import print_card\n",
        "\n",
        "# Make environment\n",
        "# Set 'record_action' to True because we need it to print results\n",
        "env = rlcard.make('leduc-holdem', config={'record_action': True})\n",
        "human_agent = HumanAgent(env.action_num)\n",
        "env.set_agents([human_agent, agent])\n",
        "\n",
        "print(\">> Leduc Hold'em pre-trained model\")\n",
        "\n",
        "while True:\n",
        "    print(\">> Start a new game\")\n",
        "\n",
        "    trajectories, payoffs = env.run(is_training=False)\n",
        "    # If the human does not take the final action, we need to\n",
        "    # print other players action\n",
        "    final_state = trajectories[0][-1][-2]\n",
        "    action_record = final_state['action_record']\n",
        "    state = final_state['raw_obs']\n",
        "    _action_list = []\n",
        "    for i in range(1, len(action_record)+1):\n",
        "        if action_record[-i][0] == state['current_player']:\n",
        "            break\n",
        "        _action_list.insert(0, action_record[-i])\n",
        "    for pair in _action_list:\n",
        "        print('>> Player', pair[0], 'chooses', pair[1])\n",
        "\n",
        "    # Let's take a look at what the agent card is\n",
        "    print('===============     CFR Agent    ===============')\n",
        "    print_card(env.get_perfect_information()['hand_cards'][1])\n",
        "\n",
        "    print('===============     Result     ===============')\n",
        "    if payoffs[0] > 0:\n",
        "        print('You win {} chips!'.format(payoffs[0]))\n",
        "    elif payoffs[0] == 0:\n",
        "        print('It is a tie.')\n",
        "    else:\n",
        "        print('You lose {} chips!'.format(-payoffs[0]))\n",
        "    print('')\n",
        "\n",
        "    input(\"Press any key to continue...\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">> Leduc Hold'em pre-trained model\n",
            ">> Start a new game\n",
            "\n",
            "=============== Community Card ===============\n",
            "┌─────────┐\n",
            "│░░░░░░░░░│\n",
            "│░░░░░░░░░│\n",
            "│░░░░░░░░░│\n",
            "│░░░░░░░░░│\n",
            "│░░░░░░░░░│\n",
            "│░░░░░░░░░│\n",
            "│░░░░░░░░░│\n",
            "└─────────┘\n",
            "===============   Your Hand    ===============\n",
            "┌─────────┐\n",
            "│J        │\n",
            "│         │\n",
            "│         │\n",
            "│    ♠    │\n",
            "│         │\n",
            "│         │\n",
            "│        J│\n",
            "└─────────┘\n",
            "===============     Chips      ===============\n",
            "Yours:   +\n",
            "Agent 1: ++\n",
            "=========== Actions You Can Choose ===========\n",
            "0: call, 1: raise, 2: fold\n",
            "\n",
            ">> You choose action (integer): 0\n",
            ">> Player 1 chooses check\n",
            "\n",
            "=============== Community Card ===============\n",
            "┌─────────┐\n",
            "│Q        │\n",
            "│         │\n",
            "│         │\n",
            "│    ♠    │\n",
            "│         │\n",
            "│         │\n",
            "│        Q│\n",
            "└─────────┘\n",
            "===============   Your Hand    ===============\n",
            "┌─────────┐\n",
            "│J        │\n",
            "│         │\n",
            "│         │\n",
            "│    ♠    │\n",
            "│         │\n",
            "│         │\n",
            "│        J│\n",
            "└─────────┘\n",
            "===============     Chips      ===============\n",
            "Yours:   ++\n",
            "Agent 1: ++\n",
            "=========== Actions You Can Choose ===========\n",
            "0: raise, 1: fold, 2: check\n",
            "\n",
            ">> You choose action (integer): 2\n",
            ">> Player 1 chooses raise\n",
            "\n",
            "=============== Community Card ===============\n",
            "┌─────────┐\n",
            "│Q        │\n",
            "│         │\n",
            "│         │\n",
            "│    ♠    │\n",
            "│         │\n",
            "│         │\n",
            "│        Q│\n",
            "└─────────┘\n",
            "===============   Your Hand    ===============\n",
            "┌─────────┐\n",
            "│J        │\n",
            "│         │\n",
            "│         │\n",
            "│    ♠    │\n",
            "│         │\n",
            "│         │\n",
            "│        J│\n",
            "└─────────┘\n",
            "===============     Chips      ===============\n",
            "Yours:   ++\n",
            "Agent 1: ++++++\n",
            "=========== Actions You Can Choose ===========\n",
            "0: call, 1: raise, 2: fold\n",
            "\n",
            ">> You choose action (integer): 2\n",
            ">> Player 0 chooses fold\n",
            "===============     CFR Agent    ===============\n",
            "┌─────────┐\n",
            "│J        │\n",
            "│         │\n",
            "│         │\n",
            "│    ♥    │\n",
            "│         │\n",
            "│         │\n",
            "│        J│\n",
            "└─────────┘\n",
            "===============     Result     ===============\n",
            "You lose 1.0 chips!\n",
            "\n",
            "Press any key to continue...\n",
            ">> Start a new game\n",
            "\n",
            "=============== Community Card ===============\n",
            "┌─────────┐\n",
            "│░░░░░░░░░│\n",
            "│░░░░░░░░░│\n",
            "│░░░░░░░░░│\n",
            "│░░░░░░░░░│\n",
            "│░░░░░░░░░│\n",
            "│░░░░░░░░░│\n",
            "│░░░░░░░░░│\n",
            "└─────────┘\n",
            "===============   Your Hand    ===============\n",
            "┌─────────┐\n",
            "│J        │\n",
            "│         │\n",
            "│         │\n",
            "│    ♠    │\n",
            "│         │\n",
            "│         │\n",
            "│        J│\n",
            "└─────────┘\n",
            "===============     Chips      ===============\n",
            "Yours:   +\n",
            "Agent 1: ++\n",
            "=========== Actions You Can Choose ===========\n",
            "0: call, 1: raise, 2: fold\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-149-50fa841efc56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\">> Start a new game\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mtrajectories\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpayoffs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;31m# If the human does not take the final action, we need to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# print other players action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/3.7.9/lib/python3.7/site-packages/rlcard/envs/env.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, is_training)\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0;31m# Agent plays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m                 \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mplayer_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mplayer_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/3.7.9/lib/python3.7/site-packages/rlcard/agents/leduc_holdem_human_agent.py\u001b[0m in \u001b[0;36meval_step\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mprobs\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0maction\u001b[0m \u001b[0mprobabilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         '''\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_print_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_record\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/3.7.9/lib/python3.7/site-packages/rlcard/agents/leduc_holdem_human_agent.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     26\u001b[0m         '''\n\u001b[1;32m     27\u001b[0m         \u001b[0m_print_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'raw_obs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'action_record'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'>> You choose action (integer): '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0maction\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'legal_actions'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Action illegel...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/3.7.9/lib/python3.7/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m         )\n\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/3.7.9/lib/python3.7/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_qxmwDxFDQz"
      },
      "source": [
        "eval_env = rlcard.make(\"leduc-holdem\", config={\"seed\": SEED, \"allow_step_back\": True})\n",
        "eval_env.set_agents([models.load(\"leduc-holdem-cfr\").agents[0], models.load(\"leduc-holdem-nfsp\").agents[0]])\n",
        "print(tournament(eval_env, 100000))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8CmAkGKFDQ0"
      },
      "source": [
        "class MCCFRAgent:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    \n",
        "    def train(self, num_iter):\n",
        "        pass\n",
        "    \n",
        "    def eval_step(self, state):\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving Poker using CFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import rlcard\n",
    "from pprint import pprint\n",
    "from rlcard import models\n",
    "from rlcard.agents import RandomAgent\n",
    "from rlcard.agents.best_response_agent import BRAgent\n",
    "from rlcard.core import Card\n",
    "from rlcard.utils import tournament\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "from env import LeducholdemEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "HANDS = [\"SJ\", \"SQ\", \"SK\", \"HJ\", \"HQ\", \"HK\"]\n",
    "HAND2IDX = {hand: i for i, hand in enumerate(HANDS)}\n",
    "\n",
    "\n",
    "def get_payoff(env, hand, our_id):\n",
    "    orig_hand = copy(env.game.players[1 - our_id].hand)\n",
    "    env.game.players[1 - our_id].hand = Card(suit=hand[0], rank=hand[1])\n",
    "    payoffs = env.game.get_payoffs()\n",
    "    env.game.players[1 - our_id].hand = orig_hand\n",
    "    return payoffs[our_id]\n",
    "\n",
    "\n",
    "def best_response_value(env, agent, our_id, num_sims, reach_probs):\n",
    "    if env.is_over():\n",
    "        # Fix reach probabilities by removing impossible entries,\n",
    "        # in which our card is the same as the opponent's or the\n",
    "        # public one.\n",
    "        info = env.get_perfect_information()\n",
    "        our_hand = info[\"hand_cards\"][our_id]\n",
    "        our_hand_idx = HAND2IDX[our_hand]\n",
    "        reach_probs[our_hand_idx] = 0.0\n",
    "        ri = np.array(reach_probs)\n",
    "        if info[\"public_card\"]:\n",
    "            public_card = info[\"public_card\"]\n",
    "            public_card_idx = HAND2IDX[public_card]\n",
    "            reach_probs[public_card_idx] = 0.0\n",
    "\n",
    "        if np.sum(reach_probs) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        reach_probs /= np.sum(reach_probs)\n",
    "        \n",
    "        # Compute expected value.\n",
    "        ev = 0.0\n",
    "        for hand, reach_prob in zip(HANDS, reach_probs):\n",
    "            ev += reach_prob * get_payoff(env, hand, our_id)\n",
    "        return ev\n",
    "\n",
    "    curr_player = env.get_player_id()\n",
    "    state = env.get_state(curr_player)\n",
    "    legal_actions = state[\"legal_actions\"]\n",
    "\n",
    "    if our_id == curr_player:\n",
    "        values = np.zeros(env.action_num)\n",
    "        for a in legal_actions:\n",
    "            # First, we check if this action runs into a chance node.\n",
    "            round_before = env.get_perfect_information()[\"current_round\"]\n",
    "            env.step(a)\n",
    "            round_after = env.get_perfect_information()[\"current_round\"]\n",
    "            is_chance_node = round_before != round_after and round_after == 1\n",
    "            env.step_back()\n",
    "\n",
    "            # Then, we compute the expected value of our action.\n",
    "            action_evs = []\n",
    "            for _ in range(num_sims if is_chance_node else 1):\n",
    "                env.step(a)\n",
    "                v = best_response_value(env, agent, our_id, num_sims, reach_probs)\n",
    "                action_evs.append(v)\n",
    "                env.step_back()\n",
    "            values[a] = np.mean(action_evs)\n",
    "            return np.max(values)\n",
    "    else:\n",
    "        # Compute probability of taking action with each holding in the\n",
    "        # opponent's infoset.\n",
    "        probs = np.zeros((6, 4))\n",
    "        for i, hand in enumerate(HANDS):\n",
    "            alt_state = deepcopy(state)\n",
    "            alt_state[\"raw_obs\"][\"hand\"] = hand\n",
    "            _, prob = agent.eval_step(alt_state)\n",
    "            probs[i] = prob\n",
    "\n",
    "        # Recursively compute expected value of state for player with our_id.\n",
    "        values = np.zeros(env.action_num)\n",
    "        for a in legal_actions:\n",
    "            # First, we check if this action runs into a chance node.\n",
    "            round_before = env.get_perfect_information()[\"current_round\"]\n",
    "            env.step(a)\n",
    "            round_after = env.get_perfect_information()[\"current_round\"]\n",
    "            is_chance_node = round_before != round_after and round_after == 1\n",
    "            env.step_back()\n",
    "\n",
    "            # Then, we compute the expected value of our action.\n",
    "            action_evs = []\n",
    "            for _ in range(num_sims if is_chance_node else 1):\n",
    "                env.step(a)\n",
    "                v = best_response_value(env, agent, our_id, num_sims, (probs.T)[a] * reach_probs)\n",
    "                action_evs.append(v)\n",
    "                env.step_back()\n",
    "            values[a] = np.mean(action_evs)\n",
    "        _, p = agent.eval_step(state)\n",
    "        ev = sum(p[a] * values[a] for a in legal_actions)\n",
    "        return ev\n",
    "        \n",
    "\n",
    "def exploitability(env, agent, our_id=0, num_sims=100):\n",
    "    values = []\n",
    "    for _ in range(num_sims):\n",
    "        env.reset()\n",
    "        v = best_response_value(env, agent, our_id, num_sims, reach_probs=np.ones(6))\n",
    "        values.append(v)\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value: 1.1722185185185183 +- 0.22524038519952083\n"
     ]
    }
   ],
   "source": [
    "env = LeducholdemEnv(config={\"allow_step_back\": True, \"allow_raw_data\": True, \"record_action\": True})\n",
    "values = exploitability(env, RandomAgent(4), our_id=0, num_sims=50)\n",
    "print(f\"Value: {np.mean(values)} +- {np.std(values)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.16666666666666666"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_agent = RandomAgent(4)\n",
    "br_agent = BRAgent(env, random_agent)\n",
    "env.set_agents([br_agent, random_agent])\n",
    "\n",
    "curr_player = env.get_player_id()\n",
    "br_agent.value(curr_player, env.get_state(curr_player), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'legal_actions': [0, 1, 2],\n",
       "  'obs': array([0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0.]),\n",
       "  'raw_obs': {'hand': 'SK',\n",
       "   'public_card': None,\n",
       "   'all_chips': [1, 2],\n",
       "   'my_chips': 1,\n",
       "   'legal_actions': ['call', 'raise', 'fold'],\n",
       "   'current_player': 0},\n",
       "  'raw_legal_actions': ['call', 'raise', 'fold'],\n",
       "  'action_record': []},\n",
       " 0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = LeducholdemEnv(config={\"allow_step_back\": True, \"allow_raw_data\": True, \"record_action\": True})\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SK'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.get_state(curr_player)[\"raw_obs\"][\"hand\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SK'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.get_state(0)[\"raw_obs\"][\"hand\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chips': [1, 2],\n",
       " 'public_card': None,\n",
       " 'hand_cards': ['SK', 'HJ'],\n",
       " 'current_round': 0,\n",
       " 'current_player': 0,\n",
       " 'legal_actions': ['call', 'raise', 'fold']}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.get_perfect_information()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.16666666666666666"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploitability(env, RandomAgent(4), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_infoset_string(state):\n",
    "    private_card = state[\"raw_obs\"][\"hand\"]\n",
    "    public_card = state[\"raw_obs\"][\"public_card\"] or \"Unknown\"\n",
    "    action_history = \":\".join([record[1] for record in state[\"action_record\"]])\n",
    "    return f\"{private_card}|{public_card}|{action_history}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PureCFRAgent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.regrets = {}\n",
    "        self.avg_strategy = {}\n",
    "        self.strategy = {}\n",
    "        self.use_raw = False\n",
    "        self.iteration = 0\n",
    "        self.next_traverser = 0\n",
    "    \n",
    "    def regret_matching(self, infoset, legal_actions):\n",
    "        if infoset not in self.regrets or np.all(self.regrets[infoset] <= 0):\n",
    "            action_probs = np.zeros(self.env.action_num)\n",
    "            action_probs[legal_actions] = 1.0 / len(legal_actions)\n",
    "            return action_probs\n",
    "        pos_regrets = np.maximum(self.regrets[infoset], 0)\n",
    "        return pos_regrets / np.sum(pos_regrets)\n",
    "\n",
    "    def train_step(self):\n",
    "        self.iteration += 1\n",
    "        self.env.reset()\n",
    "        self.pure_cfr()\n",
    "        self.next_traverser = 1 - self.next_traverser\n",
    "        for infoset in self.avg_strategy:\n",
    "            self.strategy[infoset] = self.avg_strategy[infoset] / np.sum(self.avg_strategy[infoset])\n",
    "        # for infoset in self.regrets:\n",
    "        #     self.regrets[infoset] = np.maximum(self.regrets[infoset], 0)\n",
    "    \n",
    "    def pure_cfr(self):\n",
    "        player = self.next_traverser\n",
    "        if self.env.is_over():\n",
    "            payoffs = self.env.get_payoffs()\n",
    "            return payoffs[player]\n",
    "\n",
    "        # Get state information.\n",
    "        curr_player = self.env.get_player_id()\n",
    "        state = self.env.get_state(curr_player)\n",
    "        infoset = make_infoset_string(state)\n",
    "        legal_actions = state[\"legal_actions\"]\n",
    "\n",
    "        # Compute action probabilities proportional to positive regrets.\n",
    "        action_probs = self.regret_matching(infoset, legal_actions)\n",
    "\n",
    "        assert np.isclose(np.sum(action_probs), 1.0), (\n",
    "            f\"Sum of action probs must be approx. 1 but is {np.sum(action_probs)}. \"\n",
    "            f\"Regrets: {self.regrets[infoset]}.\"\n",
    "        )\n",
    "        illegal_actions = ~np.in1d(range(self.env.action_num), legal_actions)\n",
    "        assert np.all(action_probs[illegal_actions] == 0), (\n",
    "            f\"Probability of illegal actions must be 0. \"\n",
    "            f\"Legal actions: {legal_actions}. \"\n",
    "            f\"Action probs: {action_probs}.\"\n",
    "        )\n",
    "\n",
    "        # Sample random action.\n",
    "        action = np.random.choice(range(self.env.action_num), p=action_probs)\n",
    "\n",
    "        if curr_player == player:\n",
    "            if infoset not in self.regrets:\n",
    "                self.regrets[infoset] = np.zeros(self.env.action_num)\n",
    "\n",
    "            # Compute action values.\n",
    "            action_values = np.zeros(self.env.action_num)\n",
    "            for a in legal_actions:\n",
    "                self.env.step(a)\n",
    "                action_values[a] = self.pure_cfr()\n",
    "                self.env.step_back()\n",
    "\n",
    "            # Update regrets.\n",
    "            for a in legal_actions:\n",
    "                self.regrets[infoset][a] += action_values[a] - action_values[action]\n",
    "\n",
    "            return action_values[action]\n",
    "        else:\n",
    "            if infoset not in self.avg_strategy:\n",
    "                self.avg_strategy[infoset] = np.zeros(env.action_num)\n",
    "\n",
    "            # Update average strategy.\n",
    "            self.avg_strategy[infoset][action] += 1\n",
    "\n",
    "            # Recurse.\n",
    "            self.env.step(action)\n",
    "            value = self.pure_cfr()\n",
    "            self.env.step_back()\n",
    "            return value\n",
    "\n",
    "    def eval_step(self, state):\n",
    "        infoset = make_infoset_string(state)\n",
    "        if infoset not in self.strategy:\n",
    "            legal_actions = state[\"legal_actions\"]\n",
    "            action_probs = np.zeros(env.action_num)\n",
    "            action_probs[legal_actions] = 1.0\n",
    "            action_probs /= len(legal_actions)\n",
    "        else:\n",
    "            action_probs = self.strategy[infoset]\n",
    "        action = np.random.choice(range(self.env.action_num), p=action_probs)\n",
    "        return action, action_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/Christopher/.pyenv/versions/3.7.9/lib/python3.7/site-packages/rlcard/models/pretrained/leduc_holdem_nfsp/model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c21d735c80f42f4810f0148cab77e60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=250000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10000 \t Avg reward: 0.3647 \t Exploitability: 1.0523848448427795 +- 0.36125083730954477\n",
      "Epoch: 20000 \t Avg reward: 0.4396 \t Exploitability: 1.0147704097394377 +- 0.39978272627302086\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 250000\n",
    "EVAL_INTERVAL = 10000\n",
    "EVAL_ITERS = 10000\n",
    "SAVE_DIR = \"./agents\"\n",
    "SAVE_FILE = \"cfr_model.pkl\"\n",
    "SAVE_PATH = os.path.join(SAVE_DIR, SAVE_FILE)\n",
    "\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "env = LeducholdemEnv(config={\"allow_step_back\": True, \"allow_raw_data\": True, \"record_action\": True})\n",
    "eval_env_1 = LeducholdemEnv(config={\"allow_step_back\": False, \"allow_raw_data\": True, \"record_action\": True})\n",
    "eval_env_2 = LeducholdemEnv(config={\"allow_step_back\": True, \"allow_raw_data\": True, \"record_action\": True})\n",
    "\n",
    "agent = PureCFRAgent(env)\n",
    "\n",
    "# eval_agent = models.load(\"leduc-holdem-cfr\").agents[0]\n",
    "eval_agent = models.load(\"leduc-holdem-nfsp\").agents[0]\n",
    "# eval_agent = RandomAgent(4)\n",
    "eval_env_1.set_agents([agent, eval_agent])\n",
    "\n",
    "for epoch in trange(1, NUM_EPOCHS + 1):\n",
    "    agent.train_step()\n",
    "    if epoch % EVAL_INTERVAL == 0:\n",
    "        avg_reward = tournament(eval_env_1, EVAL_ITERS)[0]\n",
    "        expl = exploitability(eval_env_2, agent, our_id=0, num_sims=50)\n",
    "        print(f\"Epoch: {epoch} \\t Avg reward: {avg_reward} \\t Exploitability: {np.mean(expl)} +- {np.std(expl)}\")\n",
    "        pickle.dump(agent, open(SAVE_PATH, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in trange(1, 1000000 + 1):\n",
    "    agent.train_step()\n",
    "    if epoch % EVAL_INTERVAL == 0:\n",
    "        avg_reward = tournament(eval_env_1, EVAL_ITERS)[0]\n",
    "        expl = exploitability(eval_env_2, agent, our_id=0, num_sims=25)\n",
    "        print(f\"Epoch: {epoch} \\t Avg reward: {avg_reward} \\t Exploitability: {np.mean(expl)} +- {np.std(expl)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env = rlcard.make(\"leduc-holdem\", config={\"seed\": SEED, \"allow_step_back\": True})\n",
    "eval_env.set_agents([models.load(\"leduc-holdem-cfr\").agents[0], models.load(\"leduc-holdem-nfsp\").agents[0]])\n",
    "print(tournament(eval_env, 100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.regrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = rlcard.make(\n",
    "    \"leduc-holdem\",\n",
    "    config={\n",
    "        \"allow_step_back\": True,\n",
    "        \"allow_raw_data\": True,\n",
    "    },\n",
    ")\n",
    "env.reset()\n",
    "env.get_state(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.get_perfect_information()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.set_agents([RandomAgent(env.action_num), RandomAgent(env.action_num)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.get_state(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_agent(player_id):\n",
    "    return PureCFRAgent(player_id, strategy=avg_strategy)\n",
    "\n",
    "exploitability(env, agent, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_agent(player_id):\n",
    "    return RandomAgent(action_num=env.action_num)\n",
    "\n",
    "exploitability(env, make_agent, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env = rlcard.make(\"leduc-holdem\", config={\"seed\": SEED})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfr_agent = PureCFRAgent(player_id=0, strategy=avg_strategy)\n",
    "random_agent = RandomAgent(env.action_num)\n",
    "\n",
    "eval_env.set_agents([cfr_agent, random_agent])\n",
    "\n",
    "for i in range(1000):\n",
    "    eval_env.reset()\n",
    "    trajectories, payoffs = eval_env.run(is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cfr_agent.unseen_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCCFRAgent:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def train(self, num_iter):\n",
    "        pass\n",
    "    \n",
    "    def eval_step(self, state):\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
